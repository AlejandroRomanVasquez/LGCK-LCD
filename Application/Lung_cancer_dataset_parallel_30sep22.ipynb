{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac48ef6",
   "metadata": {},
   "source": [
    "# Preeliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80129ec",
   "metadata": {},
   "source": [
    "### Rpy2 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e02c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44107efd",
   "metadata": {},
   "source": [
    "### Importing R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c78c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%R library(latentcor)\n",
    "%R library(glmnet)\n",
    "%R library(survival)\n",
    "%R library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00d949",
   "metadata": {},
   "source": [
    "### Importing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import math\n",
    "\n",
    "\n",
    "#For parallel computing\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#Number of cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "jobs=num_cores-1\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#knockpy (knockoffs)\n",
    "import knockpy\n",
    "from knockpy.knockoff_filter import KnockoffFilter\n",
    "from knockpy.knockoff_stats import data_dependent_threshhold\n",
    "\n",
    "#GGlasso (graphical Lasso)\n",
    "from gglasso.problem import glasso_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc798db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the package rpy2\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "# import R's packages\n",
    "base = importr('base')\n",
    "glmnet = importr('glmnet')\n",
    "dplyr = importr('dplyr')\n",
    "survival = importr('survival')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4df3e",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79bcb0",
   "metadata": {},
   "source": [
    "#### Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to make selections\n",
    "def make_selections(W, fdr):\n",
    "    \"\"\"\" Calculate data dependent threshhold and selections\n",
    "    returns a np.ndarray\n",
    "    \n",
    "    Parameters \n",
    "    ---------- \n",
    "    W : np.ndarray \n",
    "    fdr : float\n",
    "    \"\"\"  \n",
    "    \n",
    "    threshold = data_dependent_threshhold(W=W, fdr=fdr)\n",
    "    selected_flags = (W >= threshold).astype(\"float32\")\n",
    "    return selected_flags\n",
    "\n",
    "\n",
    "def lasso_glmnet_lambda_min(x):\n",
    "  \"\"\"\" Find the tuning lambda using the R package glmnet \n",
    "  \n",
    "  Returns a robjects.vectors.FloatVector\n",
    "  \n",
    "  Parameters \n",
    "  ---------- \n",
    "  x : pandas.DataFrame  \n",
    "  \"\"\"    \n",
    "    \n",
    "  #Convertion of the pandas dataframe to a R dataframe  \n",
    "  sim = x\n",
    "  with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    r_sim = robjects.conversion.py2rpy(sim)\n",
    "  robjects.globalenv[\"r_sim\"] = r_sim\n",
    "  \n",
    "  #Loading R libraries  \n",
    "  base = importr('base')\n",
    "  glmnet = importr('glmnet')\n",
    "  dplyr = importr('dplyr')\n",
    "  survival = importr('survival')\n",
    "\n",
    "  #Fitting the Coxâ€™s proportional hazards model employing glmnet\n",
    "  robjects.r(''' \n",
    "        X <- r_sim %>% select(-c(\"Pat_Died\", \"Pat_Overall_Survival_Months\"))\n",
    "        X_matrix <- as.matrix(X)\n",
    "        y <- r_sim %>% select(c(\"Pat_Died\", \"Pat_Overall_Survival_Months\"))\n",
    "        y_surv <- Surv(y$Pat_Overall_Survival_Months,y$Pat_Died)\n",
    "        cvfit <- cv.glmnet(X_matrix, y_surv, alpha=1, family = \"cox\", type.measure = \"C\", nfolds = 5, standardize = TRUE)\n",
    "        lambda_min_r <- as.numeric(cvfit$lambda.min)\n",
    "        ''')\n",
    "  #Tuning lambda\n",
    "  lambda_min = robjects.globalenv['lambda_min_r']  \n",
    "  \n",
    "  return lambda_min  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ace31",
   "metadata": {},
   "source": [
    "#### R functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00847404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#Transformation to get the corresponding latent normal variable for each original ordinal variable\n",
    "transformation_to_get_latent_normal_variable <- function(original_variable){\n",
    "     \n",
    "    \n",
    "  var_table <- table(original_variable) # Table object to obtain the levels of the ordinal variable\n",
    "  n_levels <- dim(var_table) #Levels of the original ordinal variable\n",
    "  n <- sum(var_table) #Number of observations\n",
    "  \n",
    "  if (n_levels > 3) \n",
    "        stop(\"Ordinal variables with more than 3 levels must be considered continuous, \n",
    "              because latentcor estimation is limited to binary or ternary ordinal variables\")  \n",
    "    \n",
    "    \n",
    "  #Condition when the original ordinal variable has two levels  \n",
    "  if(n_levels==2){\n",
    "    Xj_1 <- as.numeric(var_table[2])/n \n",
    "    fCj_1 <- qnorm(1-Xj_1) #Transformed cutoffs (latent normal level)\n",
    "    nj_1 <- (abs(fCj_1-(-3))/2)+(-3) #Middle point of the interval ( fCj_0=-inf, fCj_1), where -inf is replace by -3  \n",
    "    nj_2 <- (abs(3-fCj_1)/2) + fCj_1 #Middle point of the interval ( fCj_1, fCj_2=inf ), where inf is replace by 3  \n",
    " \n",
    "    #The latent Gaussian transformed value is chosen as the middle point of the interval ( fCj_i, fCj_{i+1} ). \n",
    "    #The extreme points -inf and inf were replace for -3 and 3, respectively.\n",
    "    transformed_variable <- original_variable\n",
    "    transformed_variable[original_variable==as.numeric(names(var_table)[1])]= nj_1\n",
    "    transformed_variable[original_variable==as.numeric(names(var_table)[2])]= nj_2\n",
    "  }\n",
    "  #Condition when the original ordinal variable has three levels    \n",
    "  else{\n",
    "    Xj_1 <- as.numeric(var_table[2]+var_table[3])/n  \n",
    "    fCj_1 <- qnorm(1-Xj_1) #Transformed cutoffs (latent normal level)\n",
    "    nj_1 <- (abs(fCj_1-(-3))/2)+(-3) #Middle point of the interval ( fCj_0=-inf, fCj_1 ), where -inf is replace by -3  \n",
    "    Xj_2 <- as.numeric(var_table[3])/n  \n",
    "    fCj_2 <- qnorm(1-Xj_2 ) #Transformed cutoffs (latent normal level)\n",
    "    nj_2 <- (abs(fCj_2-fCj_1)/2)+(fCj_1) #Middle point of the interval ( fCj_1, fCj_2 )  \n",
    "    nj_3 <- (abs(3-fCj_2)/2) + fCj_2 #Middle point of the interval ( fCj_2, fCj_3=inf ), where inf is replace by 3.  \n",
    " \n",
    "  \n",
    "    #The latent Gaussian transformed value is chosen as the middle point of the interval ( fCj_i,fCj_{i+1} ). \n",
    "    #The extreme points -inf and inf were replace for -3 and 3, respectively.\n",
    "    transformed_variable <- original_variable\n",
    "    transformed_variable[original_variable==as.numeric(names(var_table)[1])] = nj_1\n",
    "    transformed_variable[original_variable==as.numeric(names(var_table)[2])] = nj_2\n",
    "    transformed_variable[original_variable==as.numeric(names(var_table)[3])] = nj_3\n",
    "  } \n",
    "\n",
    "return(transformed_variable)\n",
    "}\n",
    "\n",
    "\n",
    "#Transformation to obtain the orginal ordinal variable from the latent Gaussian knockoff\n",
    "transformation_to_get_original_variable <- function(original_variable, normal_variable){\n",
    "\n",
    "  var_table <- table(original_variable) #Table object to obtain the levels of the original ordinal variable\n",
    "  n_levels <- dim(var_table) #Levels of the original ordinal variable\n",
    "  n <- sum(var_table) #Number of observations \n",
    "  original_varible_unique_values <- sort(unique(original_variable)) #Unique values of the original ordinal variable\n",
    "  ordinal_variable <- rep(0,length(normal_variable))\n",
    "    \n",
    "  \n",
    "  if (n_levels > 3) \n",
    "      stop(\"Ordinal variables with more than 3 levels must be considered continuous, \n",
    "              because latentcor estimation is limited to binary or ternary ordinal variables\") \n",
    "    \n",
    "  #Condition when the original ordinal variable has two levels  \n",
    "  if(n_levels==2){\n",
    "    Xj_1 <- as.numeric(var_table[2])/n\n",
    "    fCj_1 <- qnorm(1-Xj_1)  #Transformed cutoffs (latent normal level)\n",
    "      \n",
    "    #Ordinal_variable assignation depending on the transformed cutoffs\n",
    "    ordinal_variable[normal_variable<=fCj_1] <- original_varible_unique_values[1]  \n",
    "    ordinal_variable[normal_variable>fCj_1] <- original_varible_unique_values[2]  \n",
    "\n",
    "  }\n",
    "  #Condition when the original ordinal variable has three levels    \n",
    "  else {\n",
    "    Xj_1 <- as.numeric(var_table[2]+var_table[3])/n  \n",
    "    fCj_1 <- qnorm(1-Xj_1) #Transformed cutoffs (latent normal level)\n",
    "    Xj_2 <- as.numeric(var_table[3])/n\n",
    "    fCj_2 <- qnorm(1-Xj_2 ) #Transformed cutoffs (latent normal level) \n",
    "      \n",
    "    #Ordinal_variable assignation depending on the trnsformed cutoffs  \n",
    "    ordinal_variable[normal_variable<=fCj_1] <- original_varible_unique_values[1]  \n",
    "    ordinal_variable[normal_variable>fCj_1 & normal_variable<=fCj_2] <- original_varible_unique_values[2]  \n",
    "    ordinal_variable[normal_variable>fCj_2] <- original_varible_unique_values[3]  \n",
    "   \n",
    "    }\n",
    "return(ordinal_variable)    \n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#Function to identify if the variable is continuous(con) or ordinal (ord)\n",
    "column_type_identification <- function(col){ \n",
    "  if(length(unique(col)) < 4)\n",
    "  {type<-\"ord\"}\n",
    "  else\n",
    "  {type<-\"con\"} \n",
    "  return(type)\n",
    "}    \n",
    "\n",
    "\n",
    "#Function to identify if the variable is continuous(con), binary (bin) or ternary (ter)\n",
    "latentcor_type_identification <- function(col) { \n",
    "  if(length(unique(col)) == 2)\n",
    "  {type<-\"bin\"}\n",
    "  else if (length(unique(col)) == 3)\n",
    "  {type<-\"ter\"}\n",
    "  else\n",
    "  {type<-\"con\"} \n",
    "  return(type) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0e779",
   "metadata": {},
   "source": [
    "### Parameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23828a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial time\n",
    "tii = timer()\n",
    "\n",
    "\n",
    "n_cv = 5  #Cross validation folds\n",
    "M = 200 #Runs for stabilizing the lasso against CV fold assignation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1f419",
   "metadata": {},
   "source": [
    "# Loading the Lung cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8065e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing the csv file (307 rows and 1005 columns)\n",
    "Lung_data_complete = pd.read_csv(\"Patients_final_1003.csv\")\n",
    "\n",
    "#Encoding for binary variables\n",
    "Lung_data_complete['Pat_Gender'] = np.where(Lung_data_complete['Pat_Gender']==\"M\",0,1)\n",
    "Lung_data_complete['Pat_Stage_red'] = np.where(Lung_data_complete['Pat_Stage_red']==\"I_II\",0,1)\n",
    "\n",
    "\n",
    "#Removing NA's and nan's from the dataframe \n",
    "Lung_data_complete = Lung_data_complete.dropna(axis=0, how='any')\n",
    "#15 observations are removed (less than 5%) \n",
    "\n",
    "\n",
    "#4 observations that have a survival time equal to zero, which causes a problem when fitting the Cox-lasso model.\n",
    "#Therefore, these values are modified to have a survival time of 1 day (1/30)\n",
    "Lung_data_complete.loc[Lung_data_complete.Pat_Overall_Survival_Months == 0, \"Pat_Overall_Survival_Months\"] = 0.033 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8dc1b9",
   "metadata": {},
   "source": [
    "### Previous step that filters the most variable genes in terms of their variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273886fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes: \n",
    "#1) In the Lung_data data frame, the gene-expression column order is from the most variable to the less variable, \n",
    "#in terms of their variance. \n",
    "#2) The Lung_data contains the most 1000 expressed genes of a total of 20,356 genes of the\n",
    "# genomic dataset from the research of Rousseaux et al.(2013) located in the Lung Cancer Explorer (LCE) database \n",
    "# http://lce.biohpc.swmed.edu/.\n",
    "\n",
    "#Number of most expressed gene selected: \n",
    "gen_p = 289\n",
    "\n",
    "#Reduce dataframe with the most expressed genes given by gen_p \n",
    "#(there are 3 clinical variables and 2 variables associated to survival time and event type)\n",
    "Lung_data = Lung_data_complete.iloc[:,0: gen_p + 3 +2]\n",
    "\n",
    "#From Python to R\n",
    "%R -i Lung_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2f804",
   "metadata": {},
   "source": [
    "### Computing the censoring rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765b70f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "Lung_data \n",
    "Censoring <- round(100*as.numeric(table(Lung_data$Pat_Died)[1]/length(Lung_data$Pat_Died)),2)\n",
    "print(Censoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc3da0",
   "metadata": {},
   "source": [
    "# Applying the Cox's PH model with Lasso penalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f8ac7",
   "metadata": {},
   "source": [
    "### Stabilizing the lasso against CV (Roberts and Nowak, 2014)\n",
    "Fitting the penalized regression model  M times  to get M different values of the tuning parameter lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50542915",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ti = timer() #Initial time\n",
    "\n",
    "ls_Lung_data = list(range(M)) #List of pandas dataframes needed for the parallel processing\n",
    "for i in range(M):\n",
    "  ls_Lung_data[i] = Lung_data\n",
    "\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_lambdas = Parallel(n_jobs=jobs)(delayed(lasso_glmnet_lambda_min)(x) for x in ls_Lung_data)\n",
    "\n",
    "time_parallel_computing = timer() - ti #Final time of parallel computing\n",
    "print('Time (min) taken to run the parallel computing',round(time_parallel_computing/60,4))\n",
    "\n",
    "\n",
    "#Transforming the list to a numpy array\n",
    "lambdas = np.array(ls_lambdas)\n",
    "\n",
    "#From python to R\n",
    "%R -i lambdas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276ca32",
   "metadata": {},
   "source": [
    "### Creating matrices: X and y_surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daad519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 observations that have a survival time equal to zero, which causes a problem when fitting the Cox-lasso model.\n",
    "#Therefore, these values are modified to have a survival time of 1 day (1/30)\n",
    "%R Lung_data$Pat_Overall_Survival_Months[Lung_data$Pat_Overall_Survival_Months==0] <- 0.033\n",
    "\n",
    "#Matrix generation\n",
    "%R X <- Lung_data %>% select(-c(\"Pat_Died\", \"Pat_Overall_Survival_Months\"))\n",
    "%R X_matrix <- as.matrix(X)\n",
    "%R y <- Lung_data %>% select(c(\"Pat_Died\", \"Pat_Overall_Survival_Months\"))\n",
    "%R y_surv <- Surv(y$Pat_Overall_Survival_Months,y$Pat_Died)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08450d",
   "metadata": {},
   "source": [
    "### Fitting the Coxâ€™s proportional hazards model with Lasso (original predictors X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#The 50-percentile corresponds to the usual Cox-lasso\n",
    "lambda50 <- as.numeric(quantile(lambdas,probs=0.5)) \n",
    "\n",
    "#Fitting the final model with the tunned lambda\n",
    "fit <- glmnet(X_matrix,y_surv,alpha = 1, lambda = lambda50, family = \"cox\", standardize = TRUE)\n",
    "\n",
    "fit_coef <- coef(fit)\n",
    "\n",
    "#Showing the variables selected (coefficients different to zero)\n",
    "print(fit_coef[fit_coef[,1]!=0,])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b53a55",
   "metadata": {},
   "source": [
    "# Applying the Model-X knockoff methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66fc78b",
   "metadata": {},
   "source": [
    "## 1) knockoff construction using the LGCK procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e9576",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d567f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Design matrix X\n",
    "data_X = Lung_data.drop(['Pat_Died','Pat_Overall_Survival_Months'], axis=1)\n",
    "\n",
    "#Necessary elements  to calculate the truncated empirical cumulative distribution funcion (ECDF) estimator\n",
    "n = data_X.shape[0] #Number of observations\n",
    "p = data_X.shape[1] #Number of covariates\n",
    "delta_n = 1/( (4*n**(1/4))*math.sqrt(math.pi*math.log(n)) )\n",
    "\n",
    "#From Python to R\n",
    "%R -i data_X\n",
    "%R -i p\n",
    "%R -i delta_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea1543",
   "metadata": {},
   "source": [
    "### Step 1 of the LGCK procedure: Estimation of the latent correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15199229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column identification\n",
    "%R column_type <- apply(data_X, MARGIN = 2, FUN = column_type_identification)\n",
    "%R latentcor_type <- apply(data_X, MARGIN = 2, FUN = latentcor_type_identification)\n",
    "\n",
    "#Latencor estimation\n",
    "%R latentcor_hat <- latentcor(data_X, type=latentcor_type, method=\"original\")$R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdbc769",
   "metadata": {},
   "source": [
    "### Step 2 of the LGCK procedure: Estimation of the precision matrix of the latent correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd771c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Python to R\n",
    "%R -o latentcor_hat \n",
    "\n",
    "#Instantiate the  glasso_problem\n",
    "P = glasso_problem(S=latentcor_hat, N=n, reg_params = {'lambda1': 0.05}, latent = False, do_scaling = False)\n",
    "\n",
    "# Next, do model selection by solving the problem on a range of lambda values.\n",
    "lambda1_range = np.logspace(1, -5, 30)\n",
    "modelselect_params = {'lambda1_range': lambda1_range}\n",
    "P.model_selection(modelselect_params = modelselect_params, method = 'eBIC', gamma = 0.1)\n",
    "\n",
    "#Precision and Sigma matrices\n",
    "sol = P.solution.precision_\n",
    "Sigma_hat = np.linalg.inv(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4268b4b",
   "metadata": {},
   "source": [
    "### Step 3 of the LGCK procedure: Nonparametric transformation strategy to obtain marginal normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342960cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#Matrices for ECDF and X_norm_hat (contains the sample Gaussian Knockoffs)\n",
    "X_ecdf <- data_X \n",
    "X_norm_hat <- data_X\n",
    "\n",
    "#Empirical cumulative distribution function\n",
    "for(i in 1:p) {  \n",
    "    if(column_type[i]==\"con\"){ \n",
    "        X_ecdf[,i] <- as.vector(ecdf(data_X[,i])(data_X[,i])) \n",
    "    }\n",
    "}\n",
    "\n",
    "#Truncation for computing the Winsorized ECDF\n",
    "for(i in 1:p) {   \n",
    "    if(column_type[i]==\"con\"){ \n",
    "        X_ecdf[,i][ X_ecdf[,i] < delta_n] <- delta_n \n",
    "    }\n",
    "}\n",
    "for(i in 1:p) {   \n",
    "    if(column_type[i]==\"con\"){ \n",
    "        X_ecdf[,i][ X_ecdf[,i] > (1-delta_n)] <- 1-delta_n \n",
    "    }\n",
    "}   \n",
    "\n",
    "#Getting normal margins for continuous variables\n",
    "for(i in 1:p) {   \n",
    "    if(column_type[i]==\"con\"){ \n",
    "        X_norm_hat[,i] <- as.vector(qnorm( X_ecdf[,i] ) )\n",
    "    }\n",
    "}\n",
    "  \n",
    "#Transformation to get the corresponding latent normal variable for each ordinal variable\n",
    "for(i in 1:p) {   \n",
    "    if(column_type[i]==\"ord\"){ \n",
    "        X_norm_hat[,i] <- transformation_to_get_latent_normal_variable(data_X[,i])\n",
    "    }\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90395524",
   "metadata": {},
   "source": [
    "### Step 4 of the LGCK procedure. Sampling Gaussian knockoffs using the MRV approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From R to Python\n",
    "%R -o X_norm_hat\n",
    "\n",
    "#set seed\n",
    "np.random.seed(1)\n",
    "\n",
    "#Instantiating an object of the class GaussianSampler for sampling \n",
    "#Gaussian knockoffs using the estimated Sigma_hat and the method mvr\n",
    "Gaussian_sampler_hat = knockpy.knockoffs.GaussianSampler(X_norm_hat.to_numpy(), mu=None,\n",
    "                                                           Sigma=Sigma_hat,\n",
    "                                                           method='mvr', verbose=False)\n",
    "#Samplign the Gaussian Knockoffs\n",
    "Xk_norm_hat = Gaussian_sampler_hat.sample_knockoffs() \n",
    "\n",
    "\n",
    "#Creating a dataframes from the array that contains the Gaussian Knockoffs\n",
    "df_Xk_norm_hat = pd.DataFrame(Xk_norm_hat)\n",
    "\n",
    "#From Python to R\n",
    "%R -i df_Xk_norm_hat\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee72403",
   "metadata": {},
   "source": [
    "### Step 5 of the LGCK procedure. Reversing transformation to obtain the non-Gaussian Knockoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe to save the transformations\n",
    "%R df_Xk_hat <- df_Xk_norm_hat\n",
    "\n",
    "#Transformation to obtain the original continuous variable from the Gaussian knockoff  \n",
    "%R for(i in 1:p) {   if(column_type[i]==\"con\"){ df_Xk_hat[,i] <- as.vector(quantile(data_X[,i], probs=pnorm(df_Xk_norm_hat[,i]), type=8)) }}\n",
    "#Transformation to obtain the orginal ordinal variable from the latent Gaussian knockoff\n",
    "%R for(i in 1:p) {   if(column_type[i]==\"ord\"){ df_Xk_hat[,i] <- transformation_to_get_original_variable(original_variable=data_X[,i], normal_variable=df_Xk_norm_hat[,i])}}\n",
    "\n",
    "#From R to Python\n",
    "%R -o df_Xk_hat\n",
    "df_Xk_hat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Creating the names for the variables in Xk\n",
    "columns_names = list(data_X.columns)\n",
    "k_columns_names = ['K_'+ str(name) for name in columns_names]\n",
    "df_Xk_hat.columns = k_columns_names\n",
    "\n",
    "#Creating the dataset (X, Xk) (original variables + knockoffs)\n",
    "data_X.reset_index(drop=True, inplace=True)\n",
    "data_X_Xk = pd.concat([data_X, df_Xk_hat], axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87eb10",
   "metadata": {},
   "source": [
    "## 2) Knockoff statistic estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9da288",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating the Lung_data_Xk dataframe (Lung_data + Knockoffs Xk)Â¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the survival time and event indicator\n",
    "y_surv = Lung_data.filter(['Pat_Died','Pat_Overall_Survival_Months'], axis=1)\n",
    "y_surv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#New dataset with the knockoffs\n",
    "Lung_data_Xk = pd.concat([y_surv, data_X_Xk], axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ece2e",
   "metadata": {},
   "source": [
    "### Stabilizing the lasso against CV (Roberts and Nowak, 2014)\n",
    "Fitting the penalized regression model  M times to get M different values of the tuning parameter lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = timer() #Initial time\n",
    "\n",
    "ls_Lung_data_Xk = list(range(M)) #List of pandas data frames needed for the parallel processing\n",
    "for i in range(M):\n",
    "  ls_Lung_data_Xk[i] = Lung_data_Xk\n",
    "\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_lambdas_Xk = Parallel(n_jobs=jobs)(delayed(lasso_glmnet_lambda_min)(x) for x in ls_Lung_data_Xk)\n",
    "\n",
    "\n",
    "time_parallel_computing = timer() - ti #Final time of parallel computing\n",
    "print('Time (min) taken to run the parallel computing',round(time_parallel_computing/60,4))\n",
    "\n",
    "\n",
    "#Transforming the list to a numpy array\n",
    "lambdas_Xk = np.array(ls_lambdas_Xk)\n",
    "\n",
    "#From Python to R\n",
    "%R -i lambdas_Xk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da1cb7",
   "metadata": {},
   "source": [
    "### Creating the matrix (X,Xk) (Original variables X + Knockoffs Xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Python to R\n",
    "%R -i Lung_data_Xk\n",
    "\n",
    "#Matrix generation\n",
    "%R X_Xk <- Lung_data_Xk %>% select(-c(\"Pat_Died\", \"Pat_Overall_Survival_Months\"))\n",
    "%R X_Xk_matrix <- as.matrix(X_Xk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f4f8f",
   "metadata": {},
   "source": [
    "### Fitting the Coxâ€™s proportional hazards model with Lasso (Original variables X + Knockoffs Xk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#The 50-percentile corresponds to the usual Cox-lasso\n",
    "lambda50_Xk <- as.numeric(quantile(lambdas_Xk,probs=0.5))\n",
    "\n",
    "#Fitting the final model with the tunned lambda\n",
    "fit_LCGK <- glmnet(X_Xk_matrix, y_surv, alpha = 1, lambda =lambda50_Xk, family = \"cox\", standardize = TRUE)\n",
    "\n",
    "fit_LCGK_coef <- coef(fit_LCGK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3a45a",
   "metadata": {},
   "source": [
    "###  Computing the variable importance statistic (Z) and the knockoff feature statistic (Wj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the sparse matrix to a vector\n",
    "%R fit_LCGK_coef_vec <- as.vector(fit_LCGK_coef)\n",
    "\n",
    "#From R to Python\n",
    "%R -o fit_LCGK_coef_vec\n",
    "\n",
    "#Variable importance statistic\n",
    "Z = fit_LCGK_coef_vec\n",
    "\n",
    "#Knockoff statistic (Wj)\n",
    "pair_W = np.abs(Z[0:p]) - np.abs(Z[p:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14239cc7",
   "metadata": {},
   "source": [
    "## 3) Data-dependent threshold calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2934bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Target false discovery rate\n",
    "FDR=0.25\n",
    "\n",
    "#Data-dependent threshold calculation\n",
    "threshold = data_dependent_threshhold(W=pair_W, fdr= FDR)\n",
    "print(\"Threshold for knockoffs \")\n",
    "print(threshold)\n",
    "rejections = make_selections(W=pair_W, fdr= FDR)\n",
    "\n",
    "#Number of selections (rejections of the null hypothesis of Y independent of Xj given X-j)\n",
    "Number_Rejections_knockoff_hat = rejections.sum()\n",
    "print(\"Number of non-zero knockoff coefficients: {}\".format(Number_Rejections_knockoff_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411848ec",
   "metadata": {},
   "source": [
    "# Results of selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b9724",
   "metadata": {},
   "source": [
    "### Selected variables (LGCK-LCD procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91901a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X.columns.values[rejections==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52161a",
   "metadata": {},
   "source": [
    "### Selected variables (Cox-lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Showing the variables selected (coefficients different to zero)\n",
    "print(fit_coef[fit_coef[,1]!=0,])\n",
    "print(length(fit_coef[fit_coef[,1]!=0,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of features (p):',p) #Number of features of the initial dataset: lung_data\n",
    "\n",
    "tff = timer()\n",
    "print('Time (min) taken to run all is:',round((tff-tii)/60,4)) #Computing time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
