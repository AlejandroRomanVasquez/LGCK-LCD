{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80129ec",
   "metadata": {},
   "source": [
    "### Rpy2 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e02c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44107efd",
   "metadata": {},
   "source": [
    "### Importing R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c78c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%R library(latentcor)\n",
    "%R library(sn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00d949",
   "metadata": {},
   "source": [
    "### Importing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "#Number of cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "jobs=num_cores-1\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#knockpy\n",
    "import knockpy\n",
    "from knockpy.knockoff_filter import KnockoffFilter\n",
    "from knockpy.knockoff_stats import data_dependent_threshhold\n",
    "\n",
    "#GGlasso\n",
    "from gglasso.problem import glasso_problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the package rpy2\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "# import R's packages\n",
    "base = importr('base')\n",
    "glmnet = importr('glmnet')\n",
    "dplyr = importr('dplyr')\n",
    "survival = importr('survival')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4df3e",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to make selections\n",
    "def make_selections(W, fdr):\n",
    "    \"\"\"\" Calculate data dependent threshhold and selections \"\"\"\n",
    "    threshold = data_dependent_threshhold(W=W, fdr=fdr)\n",
    "    selected_flags = (W >= threshold).astype(\"float32\")\n",
    "    return selected_flags\n",
    "\n",
    "\n",
    "def glasso_function(x):\n",
    "    \n",
    "    #Instantiate the  glasso_problem\n",
    "    x = np.array(x)\n",
    "    P = glasso_problem(x, N=n, reg_params = {'lambda1': 0.05}, latent = False, do_scaling = False)\n",
    "\n",
    "    # Next, do model selection by solving the problem on a range of lambda values.\n",
    "    lambda1_range = np.logspace(1, -5, 30)\n",
    "    modelselect_params = {'lambda1_range': lambda1_range}\n",
    "    P.model_selection(modelselect_params = modelselect_params, method = 'eBIC', gamma = 0.1)\n",
    "\n",
    "    #Precision and Sigma matrices\n",
    "    sol = P.solution.precision_\n",
    "    return np.linalg.inv(sol)   \n",
    "\n",
    "def lasso_glmnet(x):\n",
    "    \n",
    "  #Transformation to a pandas data.frame\n",
    "  x = pd.DataFrame(x)\n",
    "  \n",
    "  #Convertion of the pandas dataframe to a R dataframe  \n",
    "  sim = x\n",
    "  with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    r_sim = robjects.conversion.py2rpy(sim)\n",
    "  robjects.globalenv[\"r_sim\"] = r_sim\n",
    "  \n",
    "  #Loading R libraries  \n",
    "  base = importr('base')\n",
    "  glmnet = importr('glmnet')\n",
    "  dplyr = importr('dplyr')\n",
    "  survival = importr('survival')\n",
    "\n",
    "  #Fitting the Cox’s proportional hazards model employing GLMNET\n",
    "  robjects.r('''\n",
    "        X <- r_sim %>% select(-c(\"Status\", \"Survival_time\"))\n",
    "        X_matrix <- as.matrix(X)\n",
    "        y <- r_sim %>% select(c(\"Status\", \"Survival_time\"))\n",
    "        y_surv <- Surv(y$Survival_time,y$Status)\n",
    "        cvfit <- cv.glmnet(X_matrix, y_surv, alpha=1, family = \"cox\", type.measure = \"C\", nfolds = 5, standardize = TRUE)\n",
    "        fit <- glmnet(X_matrix,y_surv, alpha = 1, lambda =cvfit$lambda.min, family = \"cox\",standardize = TRUE)\n",
    "        fit_coef <- coef(fit)\n",
    "        fit_coef_vec <- as.vector(fit_coef)\n",
    "        ''')\n",
    "  #Coefficients of the best model \n",
    "  r_fit_coef_vec = robjects.globalenv['fit_coef_vec']  \n",
    "  #Transformation to a numpy array\n",
    "  fit_coef_vec = np.array(r_fit_coef_vec)\n",
    "    \n",
    "  return fit_coef_vec  \n",
    "\n",
    "def latentcor_estimation(x, types):\n",
    "    \n",
    "  #Transformation to a pandas data.frame\n",
    "  x = pd.DataFrame(x)\n",
    "  \n",
    "  #Convertion of the pandas dataframe to a R dataframe  \n",
    "  sim = x\n",
    "  with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    r_sim = robjects.conversion.py2rpy(sim)\n",
    "  \n",
    "  robjects.globalenv[\"r_sim\"] = r_sim\n",
    "  robjects.globalenv[\"types\"] = types\n",
    "  \n",
    "  #Loading R libraries  \n",
    "  base = importr('base')\n",
    "  latentcor = importr('latentcor')\n",
    "  dplyr = importr('dplyr')\n",
    "\n",
    "  #Fitting the \n",
    "  robjects.r('''\n",
    "        latentcor_hat_r <- latentcor(r_sim , types = types, method=\"original\")$R  \n",
    "        ''')\n",
    "      \n",
    "  #Matrix transformation to a numpy array\n",
    "  latentcor_hat_r = robjects.globalenv['latentcor_hat_r']\n",
    "  latentcor_hat = np.array(latentcor_hat_r)\n",
    "\n",
    "  return latentcor_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c52d7",
   "metadata": {},
   "source": [
    "### Simulation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_initial = timer()\n",
    "\n",
    "n = 300 #Number of observations\n",
    "p = 300 #Number of variables\n",
    "\n",
    "p_bin = 20 #Number of binary covariates\n",
    "\n",
    "#Active covariates\n",
    "p_nonnull_cont = 35\n",
    "p_nonnull_ordinal = 5 \n",
    "\n",
    "#Number of simulations\n",
    "nsim = 200\n",
    "\n",
    "n_cv = 5  #Cross validation\n",
    "FDR = 0.2 #False discovery rate\n",
    "\n",
    "#Survival Weibull distribution parameters (scale --> sigma, shape --> nu, lambda=1/(scale^shape))\n",
    "lambda_T = 0.05 # lamnda=1/(scale^shape)=1/(sigma^nu)\n",
    "nu_T = 1.5 # With nu_T=1(shape_T=1) we have the exponential distribution \n",
    "\n",
    "#Beta coefficients\n",
    "beta_coef_1 =  np.array([ 1,  1, -1, -1,  1,  1, -1,  1, -1, -1,  1, -1,  1, \n",
    "         1, -1,  -1,  1, -1, -1,  1, -1,  1, -1,   1, -1,  1,\n",
    "       -1,  1, -1,  1, -1, -1,  1,  1,  1,  1, -1, -1,   1, -1])\n",
    "\n",
    "beta_coef = beta_coef_1\n",
    "#To change the coefficient's magnitud\n",
    "beta_coef = np.where(beta_coef_1==1, 0.7, -0.7)\n",
    "\n",
    "#Censored upper limit\n",
    "#u_max=88\n",
    "\n",
    "\n",
    "#Number of elements by block\n",
    "b_by_block = 10 # 10 for size p=500 \n",
    "\n",
    "#Correlation AR1(by blocks)\n",
    "rho = 0.6\n",
    "\n",
    "#Skew-t distribution\n",
    "skew_dist = \"Yes\" # values [\"Yes\", \"No\"]\n",
    "alpha_level = 2\n",
    "nu_level = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f9757",
   "metadata": {},
   "source": [
    "# Censoring  10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a51d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 10000000\n",
    "\n",
    "#Censored upper limit\n",
    "u_max= 538"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c44b8",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Pyhton to R\n",
    "%R -i p\n",
    "%R -i b_by_block\n",
    "%R -i rho\n",
    "\n",
    "%R n_blocks <- p%/%b_by_block\n",
    "%R covMat <- diag(n_blocks) %x% matrix(rho^abs(matrix(1:b_by_block,b_by_block, b_by_block, byrow = TRUE) - matrix(1:b_by_block, b_by_block, b_by_block)), b_by_block, b_by_block)\n",
    "%R diag(covMat) <- 1\n",
    "\n",
    "#From R to Pyhton\n",
    "%R -o covMat\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea99f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the truncated ECFD estimator\n",
    "delta_n = 1/( (4*n**(1/4))*math.sqrt(math.pi*math.log(n)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77bf84",
   "metadata": {},
   "source": [
    "### Simulations: design matrix X and survival time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c812ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in globals():\n",
    "    del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b619255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arrays and lists to save information\n",
    "ls_simulations = list(range(nsim))\n",
    "ls_beta = list(range(nsim))\n",
    "ls_X = list(range(nsim))\n",
    "ls_types = list(range(nsim))\n",
    "np_censoring = np.zeros(nsim)\n",
    "np_mean_survival_time = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Set seed for replication\n",
    "  np.random.seed(i+replication)  #Python\n",
    "  %R -i i\n",
    "  %R -i replication\n",
    "  %R set.seed(i+replication)  #R\n",
    "\n",
    "  #Sigma assignation\n",
    "  Sigma = covMat\n",
    "\n",
    "  #From Python to R \n",
    "  %R -i Sigma\n",
    "  %R -i n\n",
    "  %R -i p_bin\n",
    "  %R -i p  \n",
    "  %R -i p_nonnull_ordinal \n",
    "  %R -i p_nonnull_cont \n",
    "  %R -i alpha_level\n",
    "  %R -i nu_level\n",
    "    \n",
    "  #Creation of the vector with correlation information\n",
    "  %R lowerpart <- lower.tri(Sigma)\n",
    "  %R rhos <- Sigma[lowerpart] \n",
    "  %R p_con <- p - p_bin\n",
    "\n",
    "\n",
    "  #Binary and continiuos variables\n",
    "  %R binary <- rep(\"bin\", p_bin)\n",
    "  %R con <- rep(\"con\", p_con)\n",
    "  %R types <- sample(c(binary, con))\n",
    "  \n",
    "  #Simulations of a mixed random vector\n",
    "  %R X_norm_bin <- as.data.frame(gen_data(n = n, types = types, rhos = rhos, copulas=\"no\", XP = NULL, showplot = FALSE)$X)\n",
    "  %R X <- X_norm_bin\n",
    "\n",
    "  if (skew_dist==\"Yes\"):  \n",
    "    #There is seldom an error using the default solver \"NB\" for the function qsn().\n",
    "    #Thus, in case of error, the solver is switched to \"RFB\"\n",
    "    %R for(i in 1:p) {   if(types[i]==\"con\"){ tryCatch({X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level,nu=nu_level))}, error=function(e){X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level, nu=nu_level, solver=\"RFB\"))}) }}\n",
    "      \n",
    "  #Creating a dataframe using the design matrix X\n",
    "  #From R to Pyhton\n",
    "  %R -o X \n",
    "  %R -o types\n",
    "  %R -o p_con\n",
    "  \n",
    "  X.reset_index(drop=True, inplace=True)\n",
    "  ls_X[i] = X\n",
    "  ls_types[i] = types\n",
    "  df_X = pd.DataFrame(X)\n",
    "\n",
    "  #Names for the variables (X)\n",
    "  numbers = np.arange(1,p+1)\n",
    "  var_names = ['Var'+ str(number) for number in numbers]\n",
    "  df_X.columns= var_names\n",
    "\n",
    "  #Survival time simulation\n",
    "\n",
    "  #Extracting the ordinal and continuous variables\n",
    "  %R col_ind_ordinal <- sapply(X, function(col) length(unique(col)) < 4)\n",
    "  %R col_ind_cont <- sapply(X, function(col) length(unique(col)) > 4)\n",
    "  %R col_names <- names(X)\n",
    "  %R col_names_ordinal <- col_names[col_ind_ordinal]\n",
    "  %R col_names_cont <- col_names[col_ind_cont]  \n",
    "\n",
    "  #Variables and coefficients nonnull\n",
    "  if (p_nonnull_ordinal==0):\n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE))\n",
    "  else:\n",
    "      #If there are ordinal nonnull variables the proportion of nonnull continuous and nonnull ordinal\n",
    "      #remains the same between diferent simulated data sets  \n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE), sample(col_names_ordinal, size=p_nonnull_ordinal, replace=FALSE))\n",
    "   \n",
    "  %R ind_betas <- as.numeric(substr(col_nonnull,start=2,stop=4))\n",
    "  %R -o ind_betas \n",
    "  ind_betas_sort = np.sort(ind_betas.astype(int))\n",
    "  \n",
    "  #betas and magnitud\n",
    "  beta = np.zeros(p,dtype=float)\n",
    "  beta[ind_betas_sort-1] = beta_coef\n",
    "  ls_beta[i] = beta\n",
    "\n",
    "  #Survival time simulations (Bender et al. 2006)\n",
    "  t = ( - (np.log(np.random.uniform(low=0.0, high=1.0, size=n)) )/( lambda_T*np.exp( np.dot(X, beta) ) ))**(1/nu_T)\n",
    "\n",
    "  #Censored time and censored indicator\n",
    "  t_cens = np.random.uniform(0, u_max, size=n) \n",
    "  I_cens = np.where(t<= t_cens, 1, 0)\n",
    "\n",
    "  #Observed time \n",
    "  t_obs = np.minimum(t, t_cens)    \n",
    "\n",
    "  #Creating the dataframe with survival information\n",
    "  df_Y = pd.DataFrame({ 'Status':I_cens,'Survival_time':t_obs})\n",
    "  \n",
    "  #Data frame with simulated data (Y,X)\n",
    "  ls_simulations[i] = pd.concat([df_Y,df_X], axis=1)\n",
    "\n",
    "  #Saving information\n",
    "  np_censoring[i] = np.around((1-ls_simulations[i][\"Status\"].sum()/n)*100,decimals=4)\n",
    "  np_mean_survival_time[i] = ls_simulations[i][\"Survival_time\"].mean()\n",
    "    \n",
    "  #Eliminating X (If not, X causes problems in the loop)\n",
    "  del X\n",
    "\n",
    "time_simulations_1 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b84a0a",
   "metadata": {},
   "source": [
    "Censoring's level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43683f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_censoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82170cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_censoring.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b18107",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model  with lasso penalization (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528afa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections = np.zeros(nsim)\n",
    "np_CoxLasso_Power = np.zeros(nsim)\n",
    "np_CoxLasso_FDP = np.zeros(nsim)\n",
    "np_CoxLasso_FD = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_CoxLasso = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations)\n",
    "\n",
    "for i in range(nsim):\n",
    "  fit_coef_vec = ls_coef_CoxLasso[i] \n",
    "  \n",
    "  #Variable selection of the Penalized Cox proportional hazard model  \n",
    "  np_Number_CoxLasso_Rejections[i] = np.sum(fit_coef_vec!= 0)\n",
    "  print(\"Number of non-zero coefficients: {}\".format(np_Number_CoxLasso_Rejections[i]))\n",
    "  np_rejections_CoxLasso = np.where(fit_coef_vec!= 0,1,0)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the Penalized Cox proportional hazard model  \n",
    "  np_CoxLasso_Power[i] = np.around(100*(np.dot(np_rejections_CoxLasso, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_CoxLasso_FDP[i] = np.around(100*np.dot(np_rejections_CoxLasso, ls_beta[i] == 0) / np_rejections_CoxLasso.sum(), decimals=2)\n",
    "  np_CoxLasso_FD[i] = np.around(np.dot(np_rejections_CoxLasso, ls_beta[i] == 0))\n",
    "  print(f\"The Penalized Cox proportional hazards model has discovered {np_CoxLasso_Power[i]}% of the non-nulls with a FDP of {np_CoxLasso_FDP[i]}%\")\n",
    "\n",
    "time_CoxLASSO_1 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58db48",
   "metadata": {},
   "source": [
    "### Estimation of the Latent correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb35966",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_latentcor = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_latentcor = Parallel(n_jobs=jobs)(delayed(latentcor_estimation)(x,types) for x,types in zip(ls_X,ls_types))\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Relative Frobenius norm\n",
    "  np_Frobenius_norm_latentcor[i]= np.linalg.norm(matrices_latentcor[i] -covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_latentcor_1 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb9d56",
   "metadata": {},
   "source": [
    "### Graphical Lasso estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_Sigma_hat = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_Sigma_hat = Parallel(n_jobs=jobs)(delayed(glasso_function)(x) for x in matrices_latentcor)\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "    #Relative Frobenius norm\n",
    "    np_Frobenius_norm_Sigma_hat[i]= np.linalg.norm(matrices_Sigma_hat[i]-covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "    \n",
    "time_GraphicalLASSO_1 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2abf2",
   "metadata": {},
   "source": [
    "### Knockoffs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_simulations_Xk_hat = list(range(nsim))\n",
    "\n",
    "ti = timer() # Initial time for the loop\n",
    "\n",
    "for i in range(nsim):\n",
    "\n",
    "  X = ls_X[i]\n",
    "  df_X = ls_X[i] \n",
    "  types = ls_types[i]\n",
    "  \n",
    "  #From Python to R\n",
    "  %R -i X \n",
    "  %R -i types\n",
    "  %R -i delta_n\n",
    "  \n",
    "  #Transformation of the marginal distribution to normal distribution\n",
    "  %R X_ecdf <- X  \n",
    "  %R X_norm_hat <- X\n",
    "  \n",
    "  #Empirical cumulative distribution function\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i] <- as.vector(ecdf(X[,i])(X[,i])) }}\n",
    "\n",
    "  #For truncation:\n",
    "  #Continuous variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] < delta_n] <- delta_n }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] > (1-delta_n)] <- 1-delta_n }}                                    \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_norm_hat[,i] <- as.vector(qnorm( X_ecdf[,i] ) )}}\n",
    "  #Ordinal variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ X_norm_hat[,i][X[,i]==0] = (-1)}}  \n",
    "\n",
    "  #From R to Python\n",
    "  %R -o X_norm_hat\n",
    "    \n",
    "  #Object for Gaussian knockoffs using the Sigma_hat and method mvr\n",
    "  Gaussian_sampler_hat = knockpy.knockoffs.GaussianSampler(X_norm_hat.to_numpy(), mu=None,\n",
    "                                                           Sigma=matrices_Sigma_hat[i],\n",
    "                                                           method='mvr', verbose=False)\n",
    "  Xk_norm_hat = Gaussian_sampler_hat.sample_knockoffs()  \n",
    "\n",
    "  \n",
    "  #Creating a dataframes from the knockoffs Xk_norm_hat\n",
    "  df_Xk_norm_hat = pd.DataFrame(Xk_norm_hat)\n",
    "\n",
    "  #From Python to R\n",
    "  %R -i df_Xk_norm_hat\n",
    "  \n",
    "  #Transformation of Gaussian knockoffs to the original marginal distribution\n",
    "  %R df_Xk_hat <- df_Xk_norm_hat\n",
    "   \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ df_Xk_hat[,i] <- as.vector(quantile(X[,i], probs=pnorm(df_Xk_norm_hat[,i]), type=8)) }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]>=0]= 1}}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]<0]= 0}}\n",
    "  \n",
    "  #From R to Python  \n",
    "  %R -o df_Xk_hat\n",
    "  df_Xk_hat.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "  #Creating the names for the variables in X\n",
    "  numbers = np.arange(1,p+1)\n",
    "  kvar_names = ['KVar'+ str(number) for number in numbers]\n",
    "  df_Xk_hat.columns= kvar_names     \n",
    "\n",
    "  #Final dataset\n",
    "  ls_simulations_Xk_hat[i] = pd.concat([ls_simulations[i], df_Xk_hat], axis=1)\n",
    "  \n",
    "time_knockoffs_1 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14394a3e",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model  with lasso penalization for (X,Xk_hat) (glmnet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa73bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "\n",
    "tii = timer()#Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_knockoff_hat = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations_Xk_hat)\n",
    "\n",
    "for i in range(nsim): \n",
    "    np_Number_CoxLasso_Rejections_knockoff_hat[i] = np.sum(ls_coef_knockoff_hat[i]!= 0)\n",
    "\n",
    "time_CoxLASSO_X_Xk_1 = timer() - tii "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985571a",
   "metadata": {},
   "source": [
    "### Wj using the LASSO coeficient difference statistics and threshold rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd968c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "np_knockoff_hat_Power = np.zeros(nsim)\n",
    "np_knockoff_hat_FDP = np.zeros(nsim)\n",
    "np_knockoff_hat_FD = np.zeros(nsim)\n",
    "\n",
    "for i in range(nsim): \n",
    "  \n",
    "  #feature important asignation\n",
    "  Z = ls_coef_knockoff_hat[i]\n",
    "\n",
    "  #Wj statistic\n",
    "  pair_W = np.abs(Z[0:p]) - np.abs(Z[p:])\n",
    "\n",
    "  #Threshold selection and variable selection \n",
    "  threshold = data_dependent_threshhold(W=pair_W, fdr= FDR)\n",
    "  print(\"Threshold for knockoffs \")\n",
    "  print(threshold)\n",
    "  rejections = make_selections(W=pair_W, fdr= FDR)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the knockoff procedure   \n",
    "  np_Number_Rejections_knockoff_hat[i] = rejections.sum()\n",
    "  print(\"Number of non-zero knockoff coefficients: {}\".format(np_Number_Rejections_knockoff_hat[i]))\n",
    "  np_knockoff_hat_Power[i] = np.around(100*(np.dot(rejections, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_knockoff_hat_FDP[i] = np.around(100*np.dot(rejections, ls_beta[i] == 0) / rejections.sum(), decimals=2)\n",
    "  np_knockoff_hat_FD[i] = np.around(np.dot(rejections, ls_beta[i] == 0) )\n",
    "  print(f\"The knockoff filter has discovered {np_knockoff_hat_Power[i]}% of the non-nulls with a FDP of {np_knockoff_hat_FDP[i]}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c2736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of varying feature\n",
    "np_varying_feature = np.repeat([10],repeats=nsim,axis=0)\n",
    "\n",
    "\n",
    "#Dataframe with simulated data\n",
    "df_simulations_results_1 = pd.DataFrame({'Censoring':np_varying_feature,\n",
    "                                 'Censoring indicator':np_censoring,\n",
    "                                 'Mean survival time':np_mean_survival_time,\n",
    "                                 'Relative Frobenius norm latentcor':np_Frobenius_norm_latentcor,\n",
    "                                 'Relative Frobenius norm Sigma hat':np_Frobenius_norm_Sigma_hat,\n",
    "                     'Number_CoxLasso_Rejections':np_Number_CoxLasso_Rejections, \n",
    "                     'CoxLasso_Power(%)':np_CoxLasso_Power, \n",
    "                     'CoxLasso_FDP(%)':np_CoxLasso_FDP,\n",
    "                     'CoxLasso_FD':np_CoxLasso_FD,\n",
    "                     'Number_CoxLasso_Rejections_knockoff_hat':np_Number_CoxLasso_Rejections_knockoff_hat,\n",
    "                     'Number_Rejections_knockoff_hat':np_Number_Rejections_knockoff_hat, \n",
    "                     'knockoff_hat_Power(%)':np_knockoff_hat_Power, \n",
    "                     'knockoff_hat_FDP(%)':np_knockoff_hat_FDP,\n",
    "                     'knockoff_hat_FD':np_knockoff_hat_FD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12c0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_simulations_results_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793829f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_simulations_results_1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fcd327",
   "metadata": {},
   "source": [
    "# Censoring 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7baa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 10001000\n",
    "\n",
    "#Censored upper limit\n",
    "u_max=152"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2747669",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca66fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Pyhton to R\n",
    "%R -i p\n",
    "%R -i b_by_block\n",
    "%R -i rho\n",
    "\n",
    "%R n_blocks <- p%/%b_by_block\n",
    "%R covMat <- diag(n_blocks) %x% matrix(rho^abs(matrix(1:b_by_block,b_by_block, b_by_block, byrow = TRUE) - matrix(1:b_by_block, b_by_block, b_by_block)), b_by_block, b_by_block)\n",
    "%R diag(covMat) <- 1\n",
    "\n",
    "#From R to Pyhton\n",
    "%R -o covMat\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9eb5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the truncated ECFD estimator (Reference___)\n",
    "delta_n = 1/( (4*n**(1/4))*math.sqrt(math.pi*math.log(n)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab5a89b",
   "metadata": {},
   "source": [
    "### Simulations: design matrix X and survival time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aebb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in globals():\n",
    "    del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe78d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arrays and lists to save information\n",
    "ls_simulations = list(range(nsim))\n",
    "ls_beta = list(range(nsim))\n",
    "ls_X = list(range(nsim))\n",
    "ls_types = list(range(nsim))\n",
    "np_censoring = np.zeros(nsim)\n",
    "np_mean_survival_time = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Set seed for replication of the results\n",
    "  np.random.seed(i+replication)  #Python\n",
    "  %R -i i\n",
    "  %R -i replication\n",
    "  %R set.seed(i+replication)  #R\n",
    "\n",
    "  #Sigma assignation\n",
    "  Sigma = covMat\n",
    "\n",
    "  #From Python to R \n",
    "  %R -i Sigma\n",
    "  %R -i n\n",
    "  %R -i p_bin\n",
    "  %R -i p  \n",
    "  %R -i p_nonnull_ordinal \n",
    "  %R -i p_nonnull_cont \n",
    "  %R -i alpha_level\n",
    "  %R -i nu_level\n",
    "\n",
    "  #Creation of the vector with correlation information\n",
    "  %R lowerpart <- lower.tri(Sigma)\n",
    "  %R rhos <- Sigma[lowerpart] \n",
    "  %R p_con <- p - p_bin\n",
    "\n",
    "\n",
    "  #Binary and continiuos variables\n",
    "  %R binary <- rep(\"bin\", p_bin)\n",
    "  %R con <- rep(\"con\", p_con)\n",
    "  %R types <- sample(c(binary, con))\n",
    "  \n",
    "  #Simulations of a mixed random vector\n",
    "  %R X_norm_bin <- as.data.frame(gen_data(n = n, types = types, rhos = rhos, copulas=\"no\", XP = NULL, showplot = FALSE)$X)\n",
    "  %R X <- X_norm_bin\n",
    "\n",
    "  if (skew_dist==\"Yes\"):  \n",
    "    #There is seldom an error using the default solver \"NB\" for the function qsn().\n",
    "    #Thus, in case of error, the solver is switched to \"RFB\"\n",
    "    %R for(i in 1:p) {   if(types[i]==\"con\"){ tryCatch({X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level,nu=nu_level))}, error=function(e){X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level, nu=nu_level, solver=\"RFB\"))}) }}\n",
    "      \n",
    "  #Creating a dataframe using the design matrix X\n",
    "  #From R to Pyhton\n",
    "  %R -o X \n",
    "  %R -o types\n",
    "  %R -o p_con\n",
    "  \n",
    "  X.reset_index(drop=True, inplace=True)\n",
    "  ls_X[i] = X\n",
    "  ls_types[i] = types\n",
    "  df_X = pd.DataFrame(X)\n",
    "\n",
    "  #Names for the variables (X)\n",
    "  numbers = np.arange(1,p+1)\n",
    "  var_names = ['Var'+ str(number) for number in numbers]\n",
    "  df_X.columns= var_names\n",
    "\n",
    "  #Survival time simulation\n",
    "\n",
    "  #Extracting the ordinal and continuous variables\n",
    "  %R col_ind_ordinal <- sapply(X, function(col) length(unique(col)) < 4)\n",
    "  %R col_ind_cont <- sapply(X, function(col) length(unique(col)) > 4)\n",
    "  %R col_names <- names(X)\n",
    "  %R col_names_ordinal <- col_names[col_ind_ordinal]\n",
    "  %R col_names_cont <- col_names[col_ind_cont]  \n",
    "\n",
    "  #Variables and coefficients nonnull\n",
    "  if (p_nonnull_ordinal==0):\n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE))\n",
    "  else:\n",
    "      #If there are ordinal nonnull variables the proportion of nonnull continuous and nonnull ordinal\n",
    "      #remains the same between diferent simulated data sets  \n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE), sample(col_names_ordinal, size=p_nonnull_ordinal, replace=FALSE))\n",
    "   \n",
    "  %R ind_betas <- as.numeric(substr(col_nonnull,start=2,stop=4))\n",
    "  %R -o ind_betas \n",
    "  ind_betas_sort = np.sort(ind_betas.astype(int))\n",
    "  \n",
    "  #betas and magnitud\n",
    "  beta = np.zeros(p,dtype=float)\n",
    "  beta[ind_betas_sort-1] = beta_coef\n",
    "  ls_beta[i] = beta\n",
    "\n",
    "  #Survival time simulations (Bender et al. 2006)\n",
    "  t = ( - (np.log(np.random.uniform(low=0.0, high=1.0, size=n)) )/( lambda_T*np.exp( np.dot(X, beta) ) ))**(1/nu_T)\n",
    "\n",
    "  #Censored time and Censored indicator\n",
    "  t_cens = np.random.uniform(0, u_max, size=n) \n",
    "  I_cens = np.where(t<= t_cens, 1, 0)\n",
    "\n",
    "  #Observed time \n",
    "  t_obs = np.minimum(t, t_cens)    \n",
    "\n",
    "  #Creating the dataframe with survival information\n",
    "  df_Y = pd.DataFrame({ 'Status':I_cens,'Survival_time':t_obs})\n",
    "  \n",
    "  #Data frame with simulated data (Y,X)\n",
    "  ls_simulations[i] = pd.concat([df_Y,df_X], axis=1)\n",
    "\n",
    "  #Saving information\n",
    "  np_censoring[i] = np.around((1-ls_simulations[i][\"Status\"].sum()/n)*100,decimals=4)\n",
    "  np_mean_survival_time[i] = ls_simulations[i][\"Survival_time\"].mean()\n",
    "    \n",
    "  #Eliminating X (If not, X causes problems in the loop)\n",
    "  del X\n",
    "\n",
    "time_simulations_2 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a7d60e",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections = np.zeros(nsim)\n",
    "np_CoxLasso_Power = np.zeros(nsim)\n",
    "np_CoxLasso_FDP = np.zeros(nsim)\n",
    "np_CoxLasso_FD = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_CoxLasso = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations)\n",
    "\n",
    "for i in range(nsim):\n",
    "  fit_coef_vec = ls_coef_CoxLasso[i] \n",
    "  \n",
    "  #Variable selection of the Penalized Cox proportional hazard model  \n",
    "  np_Number_CoxLasso_Rejections[i] = np.sum(fit_coef_vec!= 0)\n",
    "  print(\"Number of non-zero coefficients: {}\".format(np_Number_CoxLasso_Rejections[i]))\n",
    "  np_rejections_CoxLasso = np.where(fit_coef_vec!= 0,1,0)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the Penalized Cox proportional hazard model  \n",
    "  np_CoxLasso_Power[i] = np.around(100*(np.dot(np_rejections_CoxLasso, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_CoxLasso_FDP[i] = np.around(100*np.dot(np_rejections_CoxLasso, ls_beta[i] == 0) / np_rejections_CoxLasso.sum(), decimals=2)\n",
    "  np_CoxLasso_FD[i] = np.around(np.dot(np_rejections_CoxLasso, ls_beta[i] == 0))\n",
    "  print(f\"The Penalized Cox proportional hazards model has discovered {np_CoxLasso_Power[i]}% of the non-nulls with a FDP of {np_CoxLasso_FDP[i]}%\")\n",
    "\n",
    "time_CoxLASSO_2 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c141ab1",
   "metadata": {},
   "source": [
    "### Estimation of the Latent correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77343289",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_latentcor = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_latentcor = Parallel(n_jobs=jobs)(delayed(latentcor_estimation)(x,types) for x,types in zip(ls_X,ls_types))\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Relative Frobenius norm\n",
    "  np_Frobenius_norm_latentcor[i]= np.linalg.norm(matrices_latentcor[i] -covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_latentcor_2 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ac565",
   "metadata": {},
   "source": [
    "### Graphical lasso estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ce0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_Sigma_hat = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_Sigma_hat = Parallel(n_jobs=jobs)(delayed(glasso_function)(x) for x in matrices_latentcor)\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "    #Relative Frobenius norm\n",
    "    np_Frobenius_norm_Sigma_hat[i]= np.linalg.norm(matrices_Sigma_hat[i]-covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "    \n",
    "time_GraphicalLASSO_2 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce384872",
   "metadata": {},
   "source": [
    "### Knockoffs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_simulations_Xk_hat = list(range(nsim))\n",
    "\n",
    "ti = timer()#Initial time for the loop\n",
    "\n",
    "for i in range(nsim):\n",
    "\n",
    "  X = ls_X[i]\n",
    "  df_X = ls_X[i] \n",
    "  types = ls_types[i]\n",
    "  \n",
    "  #From Python to R\n",
    "  %R -i X \n",
    "  %R -i types\n",
    "  %R -i delta_n\n",
    "  \n",
    "  #Transformation of the marginal distribution to normal distribution\n",
    "  %R X_ecdf <- X  \n",
    "  %R X_norm_hat <- X\n",
    "  \n",
    "  #Empirical cumulative distribution function\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i] <- as.vector(ecdf(X[,i])(X[,i])) }}\n",
    "\n",
    "  #For truncation:\n",
    "  #Continuous variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] < delta_n] <- delta_n }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] > (1-delta_n)] <- 1-delta_n }}                                    \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_norm_hat[,i] <- as.vector(qnorm( X_ecdf[,i] ) )}}\n",
    "  #Ordinal variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ X_norm_hat[,i][X[,i]==0] = (-1)}}  \n",
    "\n",
    "  #From R to Python\n",
    "  %R -o X_norm_hat\n",
    "    \n",
    "  #Object for Gaussian knockoffs using the Sigma_hat and method mvr\n",
    "  Gaussian_sampler_hat = knockpy.knockoffs.GaussianSampler(X_norm_hat.to_numpy(), mu=None,\n",
    "                                                           Sigma=matrices_Sigma_hat[i],\n",
    "                                                           method='mvr', verbose=False)\n",
    "  Xk_norm_hat = Gaussian_sampler_hat.sample_knockoffs()  \n",
    "\n",
    "  \n",
    "  #Creating a dataframes from the knockoffs Xk_norm_hat\n",
    "  df_Xk_norm_hat = pd.DataFrame(Xk_norm_hat)\n",
    "\n",
    "  #From Python to R\n",
    "  %R -i df_Xk_norm_hat\n",
    "   \n",
    "  #Transformation of Gaussian knockoffs to the original marginal distribution\n",
    "  %R df_Xk_hat <- df_Xk_norm_hat\n",
    "   \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ df_Xk_hat[,i] <- as.vector(quantile(X[,i], probs=pnorm(df_Xk_norm_hat[,i]), type=8)) }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]>=0]= 1}}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]<0]= 0}}\n",
    "  \n",
    "  #From R to Python  \n",
    "  %R -o df_Xk_hat\n",
    "  df_Xk_hat.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "  #Creating the names for the variables in X\n",
    "  numbers = np.arange(1,p+1)\n",
    "  kvar_names = ['KVar'+ str(number) for number in numbers]\n",
    "  df_Xk_hat.columns= kvar_names     \n",
    "\n",
    "  #Final dataset\n",
    "  ls_simulations_Xk_hat[i] = pd.concat([ls_simulations[i], df_Xk_hat], axis=1)\n",
    "  \n",
    "time_knockoffs_2 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a39775",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization for (X,Xk_hat) (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba825291",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "\n",
    "tii = timer()#Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_knockoff_hat = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations_Xk_hat)\n",
    "\n",
    "for i in range(nsim): \n",
    "    np_Number_CoxLasso_Rejections_knockoff_hat[i] = np.sum(ls_coef_knockoff_hat[i]!= 0)\n",
    "\n",
    "time_CoxLASSO_X_Xk_2 = timer() - tii "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63da21",
   "metadata": {},
   "source": [
    "### Wj using the LASSO coeficient difference statistics and threshold rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "np_knockoff_hat_Power = np.zeros(nsim)\n",
    "np_knockoff_hat_FDP = np.zeros(nsim)\n",
    "np_knockoff_hat_FD = np.zeros(nsim)\n",
    "\n",
    "for i in range(nsim): \n",
    "  \n",
    "  #feature important asignation\n",
    "  Z = ls_coef_knockoff_hat[i]\n",
    "\n",
    "  #Wj statistic\n",
    "  pair_W = np.abs(Z[0:p]) - np.abs(Z[p:])\n",
    "\n",
    "  #Threshold selection and variable selection \n",
    "  threshold = data_dependent_threshhold(W=pair_W, fdr= FDR)\n",
    "  print(\"Threshold for knockoffs \")\n",
    "  print(threshold)\n",
    "  rejections = make_selections(W=pair_W, fdr= FDR)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the knockoff procedure   \n",
    "  np_Number_Rejections_knockoff_hat[i] = rejections.sum()\n",
    "  print(\"Number of non-zero knockoff coefficients: {}\".format(np_Number_Rejections_knockoff_hat[i]))\n",
    "  np_knockoff_hat_Power[i] = np.around(100*(np.dot(rejections, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_knockoff_hat_FDP[i] = np.around(100*np.dot(rejections, ls_beta[i] == 0) / rejections.sum(), decimals=2)\n",
    "  np_knockoff_hat_FD[i] = np.around(np.dot(rejections, ls_beta[i] == 0) )\n",
    "  print(f\"The knockoff filter has discovered {np_knockoff_hat_Power[i]}% of the non-nulls with a FDP of {np_knockoff_hat_FDP[i]}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of varying feature\n",
    "np_varying_feature = np.repeat([20],repeats=nsim,axis=0)\n",
    "\n",
    "\n",
    "#Dataframe with simulated data\n",
    "df_simulations_results_2 = pd.DataFrame({'Censoring':np_varying_feature,\n",
    "                                 'Censoring indicator':np_censoring,\n",
    "                                 'Mean survival time':np_mean_survival_time,\n",
    "                                 'Relative Frobenius norm latentcor':np_Frobenius_norm_latentcor,\n",
    "                                 'Relative Frobenius norm Sigma hat':np_Frobenius_norm_Sigma_hat,\n",
    "                     'Number_CoxLasso_Rejections':np_Number_CoxLasso_Rejections, \n",
    "                     'CoxLasso_Power(%)':np_CoxLasso_Power, \n",
    "                     'CoxLasso_FDP(%)':np_CoxLasso_FDP,\n",
    "                     'CoxLasso_FD':np_CoxLasso_FD,\n",
    "                     'Number_CoxLasso_Rejections_knockoff_hat':np_Number_CoxLasso_Rejections_knockoff_hat,\n",
    "                     'Number_Rejections_knockoff_hat':np_Number_Rejections_knockoff_hat, \n",
    "                     'knockoff_hat_Power(%)':np_knockoff_hat_Power, \n",
    "                     'knockoff_hat_FDP(%)':np_knockoff_hat_FDP,\n",
    "                     'knockoff_hat_FD':np_knockoff_hat_FD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulations_results_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b4429",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_simulations_results_2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8671b90",
   "metadata": {},
   "source": [
    "# Censoring 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff129395",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 10002000\n",
    "\n",
    "#Censored upper limit\n",
    "u_max=61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929ebdb",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Pyhton to R\n",
    "%R -i p\n",
    "%R -i b_by_block\n",
    "%R -i rho\n",
    "\n",
    "%R n_blocks <- p%/%b_by_block\n",
    "%R covMat <- diag(n_blocks) %x% matrix(rho^abs(matrix(1:b_by_block,b_by_block, b_by_block, byrow = TRUE) - matrix(1:b_by_block, b_by_block, b_by_block)), b_by_block, b_by_block)\n",
    "%R diag(covMat) <- 1\n",
    "\n",
    "#From R to Pyhton\n",
    "%R -o covMat\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91386834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the truncated ECFD estimator (Reference___)\n",
    "delta_n = 1/( (4*n**(1/4))*math.sqrt(math.pi*math.log(n)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d90f",
   "metadata": {},
   "source": [
    "### Simulations: design matrix X and survival time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in globals():\n",
    "    del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arrays and lists to save information\n",
    "ls_simulations = list(range(nsim))\n",
    "ls_beta = list(range(nsim))\n",
    "ls_X = list(range(nsim))\n",
    "ls_types = list(range(nsim))\n",
    "np_censoring = np.zeros(nsim)\n",
    "np_mean_survival_time = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Set seed for replication of the results\n",
    "  np.random.seed(i+replication)  #Python\n",
    "  %R -i i\n",
    "  %R -i replication\n",
    "  %R set.seed(i+replication)  #R\n",
    "\n",
    "  #Sigma assignation\n",
    "  Sigma = covMat\n",
    "\n",
    "  #From Python to R \n",
    "  %R -i Sigma\n",
    "  %R -i n\n",
    "  %R -i p_bin\n",
    "  %R -i p  \n",
    "  %R -i p_nonnull_ordinal \n",
    "  %R -i p_nonnull_cont \n",
    "  %R -i alpha_level\n",
    "  %R -i nu_level\n",
    "\n",
    "  #Creation of the vector with correlation information\n",
    "  %R lowerpart <- lower.tri(Sigma)\n",
    "  %R rhos <- Sigma[lowerpart] \n",
    "  %R p_con <- p - p_bin\n",
    "\n",
    "\n",
    "  #Binary and continiuos variables\n",
    "  %R binary <- rep(\"bin\", p_bin)\n",
    "  %R con <- rep(\"con\", p_con)\n",
    "  %R types <- sample(c(binary, con))\n",
    "  \n",
    "  #Simulations of a mixed random vector\n",
    "  %R X_norm_bin <- as.data.frame(gen_data(n = n, types = types, rhos = rhos, copulas=\"no\", XP = NULL, showplot = FALSE)$X)\n",
    "  %R X <- X_norm_bin\n",
    "\n",
    "  if (skew_dist==\"Yes\"):  \n",
    "    #There is seldom an error using the default solver \"NB\" for the function qsn().\n",
    "    #Thus, in case of error, the solver is switched to \"RFB\"\n",
    "    %R for(i in 1:p) {   if(types[i]==\"con\"){ tryCatch({X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level,nu=nu_level))}, error=function(e){X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level, nu=nu_level, solver=\"RFB\"))}) }}\n",
    "      \n",
    "  #Creating a dataframe using the design matrix X\n",
    "  #From R to Pyhton\n",
    "  %R -o X \n",
    "  %R -o types\n",
    "  %R -o p_con\n",
    "  \n",
    "  X.reset_index(drop=True, inplace=True)\n",
    "  ls_X[i] = X\n",
    "  ls_types[i] = types\n",
    "  df_X = pd.DataFrame(X)\n",
    "\n",
    "  #Names for the variables (X)\n",
    "  numbers = np.arange(1,p+1)\n",
    "  var_names = ['Var'+ str(number) for number in numbers]\n",
    "  df_X.columns= var_names\n",
    "\n",
    "  #Survival time simulation\n",
    "\n",
    "  #Extracting the ordinal and continuous variables\n",
    "  %R col_ind_ordinal <- sapply(X, function(col) length(unique(col)) < 4)\n",
    "  %R col_ind_cont <- sapply(X, function(col) length(unique(col)) > 4)\n",
    "  %R col_names <- names(X)\n",
    "  %R col_names_ordinal <- col_names[col_ind_ordinal]\n",
    "  %R col_names_cont <- col_names[col_ind_cont]  \n",
    "\n",
    "  #Variables and coefficients nonnull\n",
    "  if (p_nonnull_ordinal==0):\n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE))\n",
    "  else:\n",
    "      #If there are ordinal nonnull variables the proportion of nonnull continuous and nonnull ordinal\n",
    "      #remains the same between diferent simulated data sets  \n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE), sample(col_names_ordinal, size=p_nonnull_ordinal, replace=FALSE))\n",
    "   \n",
    "  %R ind_betas <- as.numeric(substr(col_nonnull,start=2,stop=4))\n",
    "  %R -o ind_betas \n",
    "  ind_betas_sort = np.sort(ind_betas.astype(int))\n",
    "  \n",
    "  #betas and magnitud\n",
    "  beta = np.zeros(p,dtype=float)\n",
    "  beta[ind_betas_sort-1] = beta_coef\n",
    "  ls_beta[i] = beta\n",
    "\n",
    "  #Survival time simulations (Bender et al. 2006)\n",
    "  t = ( - (np.log(np.random.uniform(low=0.0, high=1.0, size=n)) )/( lambda_T*np.exp( np.dot(X, beta) ) ))**(1/nu_T)\n",
    "\n",
    "  #Censored time and Censored indicator\n",
    "  t_cens = np.random.uniform(0, u_max, size=n) #With Upper=70 --> 34% of censoring\n",
    "  I_cens = np.where(t<= t_cens, 1, 0)\n",
    "\n",
    "  #Observed time \n",
    "  t_obs = np.minimum(t, t_cens)    \n",
    "\n",
    "  #Creating the dataframe with survival information\n",
    "  df_Y = pd.DataFrame({ 'Status':I_cens,'Survival_time':t_obs})\n",
    "  \n",
    "  #Data frame with simulated data (Y,X)\n",
    "  ls_simulations[i] = pd.concat([df_Y,df_X], axis=1)\n",
    "\n",
    "  #Saving information\n",
    "  np_censoring[i] = np.around((1-ls_simulations[i][\"Status\"].sum()/n)*100,decimals=4)\n",
    "  np_mean_survival_time[i] = ls_simulations[i][\"Survival_time\"].mean()\n",
    "    \n",
    "  #Eliminating X (If not, X causes problems in the loop)\n",
    "  del X\n",
    "\n",
    "time_simulations_3 = timer() - ti     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10fb75",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections = np.zeros(nsim)\n",
    "np_CoxLasso_Power = np.zeros(nsim)\n",
    "np_CoxLasso_FDP = np.zeros(nsim)\n",
    "np_CoxLasso_FD = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_CoxLasso = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations)\n",
    "\n",
    "for i in range(nsim):\n",
    "  fit_coef_vec = ls_coef_CoxLasso[i] \n",
    "  \n",
    "  #Variable selection of the Penalized Cox proportional hazard model  \n",
    "  np_Number_CoxLasso_Rejections[i] = np.sum(fit_coef_vec!= 0)\n",
    "  print(\"Number of non-zero coefficients: {}\".format(np_Number_CoxLasso_Rejections[i]))\n",
    "  np_rejections_CoxLasso = np.where(fit_coef_vec!= 0,1,0)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the Penalized Cox proportional hazard model  \n",
    "  np_CoxLasso_Power[i] = np.around(100*(np.dot(np_rejections_CoxLasso, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_CoxLasso_FDP[i] = np.around(100*np.dot(np_rejections_CoxLasso, ls_beta[i] == 0) / np_rejections_CoxLasso.sum(), decimals=2)\n",
    "  np_CoxLasso_FD[i] = np.around(np.dot(np_rejections_CoxLasso, ls_beta[i] == 0))\n",
    "  print(f\"The Penalized Cox proportional hazards model has discovered {np_CoxLasso_Power[i]}% of the non-nulls with a FDP of {np_CoxLasso_FDP[i]}%\")\n",
    "\n",
    "time_CoxLASSO_3 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68bff2",
   "metadata": {},
   "source": [
    "### Estimation of the latent correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_latentcor = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_latentcor = Parallel(n_jobs=jobs)(delayed(latentcor_estimation)(x,types) for x,types in zip(ls_X,ls_types))\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Relative Frobenius norm\n",
    "  np_Frobenius_norm_latentcor[i]= np.linalg.norm(matrices_latentcor[i] -covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_latentcor_3 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf2cd3",
   "metadata": {},
   "source": [
    "### Graphical lasso estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eee58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_Sigma_hat = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_Sigma_hat = Parallel(n_jobs=jobs)(delayed(glasso_function)(x) for x in matrices_latentcor)\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "    #Relative Frobenius norm\n",
    "    np_Frobenius_norm_Sigma_hat[i]= np.linalg.norm(matrices_Sigma_hat[i]-covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_GraphicalLASSO_3 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb46d4b",
   "metadata": {},
   "source": [
    "### Knockoffs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ffcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_simulations_Xk_hat = list(range(nsim))\n",
    "\n",
    "ti = timer() #Initial time for the loop\n",
    "\n",
    "for i in range(nsim):\n",
    "\n",
    "  X = ls_X[i]\n",
    "  df_X = ls_X[i] \n",
    "  types = ls_types[i]\n",
    "  \n",
    "  #From Python to R\n",
    "  %R -i X \n",
    "  %R -i types\n",
    "  %R -i delta_n\n",
    "  \n",
    "  #Transformation of the marginal distribution to normal distribution\n",
    "  %R X_ecdf <- X  \n",
    "  %R X_norm_hat <- X\n",
    "  \n",
    "  #Empirical cumulative distribution function\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i] <- as.vector(ecdf(X[,i])(X[,i])) }}\n",
    "\n",
    "  #For truncation:\n",
    "  #Continuous variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] < delta_n] <- delta_n }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] > (1-delta_n)] <- 1-delta_n }}                                    \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_norm_hat[,i] <- as.vector(qnorm( X_ecdf[,i] ) )}}\n",
    "  #Ordinal variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ X_norm_hat[,i][X[,i]==0] = (-1)}}  \n",
    "\n",
    "  #From R to Python\n",
    "  %R -o X_norm_hat\n",
    "    \n",
    "  #Object for Gaussian knockoffs using the Sigma_hat and method mvr\n",
    "  Gaussian_sampler_hat = knockpy.knockoffs.GaussianSampler(X_norm_hat.to_numpy(), mu=None,\n",
    "                                                           Sigma=matrices_Sigma_hat[i],\n",
    "                                                           method='mvr', verbose=False)\n",
    "  Xk_norm_hat = Gaussian_sampler_hat.sample_knockoffs()  \n",
    "\n",
    "  \n",
    "  #Creating a dataframes from the knockoffs Xk_norm_hat\n",
    "  df_Xk_norm_hat = pd.DataFrame(Xk_norm_hat)\n",
    "\n",
    "  #From Python to R\n",
    "  %R -i df_Xk_norm_hat\n",
    "   \n",
    "  #Transformation of Gaussian knockoffs to the original marginal distribution\n",
    "  %R df_Xk_hat <- df_Xk_norm_hat\n",
    "   \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ df_Xk_hat[,i] <- as.vector(quantile(X[,i], probs=pnorm(df_Xk_norm_hat[,i]), type=8)) }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]>=0]= 1}}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]<0]= 0}}\n",
    "  \n",
    "  #From R to Python  \n",
    "  %R -o df_Xk_hat\n",
    "  df_Xk_hat.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "  #Creating the names for the variables in Xk_hat\n",
    "  numbers = np.arange(1,p+1)\n",
    "  kvar_names = ['KVar'+ str(number) for number in numbers]\n",
    "  df_Xk_hat.columns= kvar_names     \n",
    "\n",
    "  #Final dataset\n",
    "  ls_simulations_Xk_hat[i] = pd.concat([ls_simulations[i], df_Xk_hat], axis=1)\n",
    "  \n",
    "time_knockoffs_3 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0efb6a",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization for (X,Xk_hat) (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "\n",
    "tii = timer()#Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_knockoff_hat = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations_Xk_hat)\n",
    "\n",
    "for i in range(nsim): \n",
    "    np_Number_CoxLasso_Rejections_knockoff_hat[i] = np.sum(ls_coef_knockoff_hat[i]!= 0)\n",
    "    \n",
    "time_CoxLASSO_X_Xk_3 = timer() - tii "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00077a",
   "metadata": {},
   "source": [
    "### Wj using the LASSO coeficient difference statistics and threshold rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd61f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "np_knockoff_hat_Power = np.zeros(nsim)\n",
    "np_knockoff_hat_FDP = np.zeros(nsim)\n",
    "np_knockoff_hat_FD = np.zeros(nsim)\n",
    "\n",
    "for i in range(nsim): \n",
    "  \n",
    "  #feature important asignation\n",
    "  Z = ls_coef_knockoff_hat[i]\n",
    "\n",
    "  #Wj statistic\n",
    "  pair_W = np.abs(Z[0:p]) - np.abs(Z[p:])\n",
    "\n",
    "  #Threshold selection and variable selection \n",
    "  threshold = data_dependent_threshhold(W=pair_W, fdr= FDR)\n",
    "  print(\"Threshold for knockoffs \")\n",
    "  print(threshold)\n",
    "  rejections = make_selections(W=pair_W, fdr= FDR)\n",
    "\n",
    "  np_Number_Rejections_knockoff_hat[i] = rejections.sum()\n",
    "  print(\"Number of non-zero knockoff coefficients: {}\".format(np_Number_Rejections_knockoff_hat[i]))\n",
    "  np_knockoff_hat_Power[i] = np.around(100*(np.dot(rejections, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_knockoff_hat_FDP[i] = np.around(100*np.dot(rejections, ls_beta[i] == 0) / rejections.sum(), decimals=2)\n",
    "  np_knockoff_hat_FD[i] = np.around(np.dot(rejections, ls_beta[i] == 0) )\n",
    "  print(f\"The knockoff filter has discovered {np_knockoff_hat_Power[i]}% of the non-nulls with a FDP of {np_knockoff_hat_FDP[i]}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of varying feature\n",
    "np_varying_feature = np.repeat([30],repeats=nsim,axis=0)\n",
    "\n",
    "\n",
    "#Dataframe with simulated data\n",
    "df_simulations_results_3 = pd.DataFrame({\"Censoring\":np_varying_feature,\n",
    "                                 'Censoring indicator':np_censoring,\n",
    "                                 'Mean survival time':np_mean_survival_time,\n",
    "                                 'Relative Frobenius norm latentcor':np_Frobenius_norm_latentcor,\n",
    "                                 'Relative Frobenius norm Sigma hat':np_Frobenius_norm_Sigma_hat,\n",
    "                     'Number_CoxLasso_Rejections':np_Number_CoxLasso_Rejections, \n",
    "                     'CoxLasso_Power(%)':np_CoxLasso_Power, \n",
    "                     'CoxLasso_FDP(%)':np_CoxLasso_FDP,\n",
    "                     'CoxLasso_FD':np_CoxLasso_FD,\n",
    "                     'Number_CoxLasso_Rejections_knockoff_hat':np_Number_CoxLasso_Rejections_knockoff_hat,\n",
    "                     'Number_Rejections_knockoff_hat':np_Number_Rejections_knockoff_hat, \n",
    "                     'knockoff_hat_Power(%)':np_knockoff_hat_Power, \n",
    "                     'knockoff_hat_FDP(%)':np_knockoff_hat_FDP,\n",
    "                     'knockoff_hat_FD':np_knockoff_hat_FD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulations_results_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79456548",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_simulations_results_3.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1c887",
   "metadata": {},
   "source": [
    "# Censoring 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67da230",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 10003000\n",
    "\n",
    "#Censored upper limit\n",
    "u_max=27.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447580d7",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Pyhton to R\n",
    "%R -i p\n",
    "%R -i b_by_block\n",
    "%R -i rho\n",
    "\n",
    "%R n_blocks <- p%/%b_by_block\n",
    "%R covMat <- diag(n_blocks) %x% matrix(rho^abs(matrix(1:b_by_block,b_by_block, b_by_block, byrow = TRUE) - matrix(1:b_by_block, b_by_block, b_by_block)), b_by_block, b_by_block)\n",
    "%R diag(covMat) <- 1\n",
    "\n",
    "#From R to Pyhton\n",
    "%R -o covMat\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the truncated ECFD estimator (Reference___)\n",
    "delta_n = 1/( (4*n**(1/4))*math.sqrt(math.pi*math.log(n)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2c32a",
   "metadata": {},
   "source": [
    "### Simulations: design matrix X and survival time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in globals():\n",
    "    del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb37703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arrays and lists to save information\n",
    "ls_simulations = list(range(nsim))\n",
    "ls_beta = list(range(nsim))\n",
    "ls_X = list(range(nsim))\n",
    "ls_types = list(range(nsim))\n",
    "np_censoring = np.zeros(nsim)\n",
    "np_mean_survival_time = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Set seed for replication\n",
    "  np.random.seed(i+replication)  #Python\n",
    "  %R -i i\n",
    "  %R -i replication\n",
    "  %R set.seed(i+replication)  #R\n",
    "\n",
    "  #Sigma assignation\n",
    "  Sigma = covMat\n",
    "\n",
    "  #From Python to R \n",
    "  %R -i Sigma\n",
    "  %R -i n\n",
    "  %R -i p_bin\n",
    "  %R -i p  \n",
    "  %R -i p_nonnull_ordinal \n",
    "  %R -i p_nonnull_cont \n",
    "  %R -i alpha_level\n",
    "  %R -i nu_level\n",
    "\n",
    "  #Creation of the vector with correlation information\n",
    "  %R lowerpart <- lower.tri(Sigma)\n",
    "  %R rhos <- Sigma[lowerpart] \n",
    "  %R p_con <- p - p_bin\n",
    "\n",
    "\n",
    "  #Binary and continiuos variables\n",
    "  %R binary <- rep(\"bin\", p_bin)\n",
    "  %R con <- rep(\"con\", p_con)\n",
    "  %R types <- sample(c(binary, con))\n",
    "  \n",
    "  #Simulations of a mixed random vector\n",
    "  %R X_norm_bin <- as.data.frame(gen_data(n = n, types = types, rhos = rhos, copulas=\"no\", XP = NULL, showplot = FALSE)$X)\n",
    "  %R X <- X_norm_bin\n",
    "\n",
    "  if (skew_dist==\"Yes\"):  \n",
    "    #There is seldom an error using the default solver \"NB\" for the function qsn().\n",
    "    #Thus, in case of error, the solver is switched to \"RFB\"\n",
    "    %R for(i in 1:p) {   if(types[i]==\"con\"){ tryCatch({X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level,nu=nu_level))}, error=function(e){X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level, nu=nu_level, solver=\"RFB\"))}) }}\n",
    "   \n",
    "  #Creating a dataframe using the design matrix X\n",
    "  #From R to Pyhton\n",
    "  %R -o X \n",
    "  %R -o types\n",
    "  %R -o p_con\n",
    "  \n",
    "  X.reset_index(drop=True, inplace=True)\n",
    "  ls_X[i] = X\n",
    "  ls_types[i] = types\n",
    "  df_X = pd.DataFrame(X)\n",
    "\n",
    "  #Names for the variables (X)\n",
    "  numbers = np.arange(1,p+1)\n",
    "  var_names = ['Var'+ str(number) for number in numbers]\n",
    "  df_X.columns= var_names\n",
    "\n",
    "  #Survival time simulation\n",
    "\n",
    "  #Extracting the ordinal and continuous variables\n",
    "  %R col_ind_ordinal <- sapply(X, function(col) length(unique(col)) < 4)\n",
    "  %R col_ind_cont <- sapply(X, function(col) length(unique(col)) > 4)\n",
    "  %R col_names <- names(X)\n",
    "  %R col_names_ordinal <- col_names[col_ind_ordinal]\n",
    "  %R col_names_cont <- col_names[col_ind_cont]  \n",
    "\n",
    "  #Variables and coefficients nonnull\n",
    "  if (p_nonnull_ordinal==0):\n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE))\n",
    "  else:\n",
    "      #If there are ordinal nonnull variables the proportion of nonnull continuous and nonnull ordinal\n",
    "      #remains the same between diferent simulated data sets  \n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE), sample(col_names_ordinal, size=p_nonnull_ordinal, replace=FALSE))\n",
    "   \n",
    "  %R ind_betas <- as.numeric(substr(col_nonnull,start=2,stop=4))\n",
    "  %R -o ind_betas \n",
    "  ind_betas_sort = np.sort(ind_betas.astype(int))\n",
    "  \n",
    "  #betas and magnitud\n",
    "  beta = np.zeros(p,dtype=float)\n",
    "  beta[ind_betas_sort-1] = beta_coef\n",
    "  ls_beta[i] = beta\n",
    "\n",
    "  #Survival time simulations (Bender et al. 2006)\n",
    "  t = ( - (np.log(np.random.uniform(low=0.0, high=1.0, size=n)) )/( lambda_T*np.exp( np.dot(X, beta) ) ))**(1/nu_T)\n",
    "\n",
    "  #Censored time and Censored indicator\n",
    "  t_cens = np.random.uniform(0, u_max, size=n) #With Upper=70 --> 34% of censoring\n",
    "  I_cens = np.where(t<= t_cens, 1, 0)\n",
    "\n",
    "  #Observed time \n",
    "  t_obs = np.minimum(t, t_cens)    \n",
    "\n",
    "  #Creating the dataframe with survival information\n",
    "  df_Y = pd.DataFrame({ 'Status':I_cens,'Survival_time':t_obs})\n",
    "  \n",
    "  #Data frame with simulated data (Y,X)\n",
    "  ls_simulations[i] = pd.concat([df_Y,df_X], axis=1)\n",
    "\n",
    "  #Saving information\n",
    "  np_censoring[i] = np.around((1-ls_simulations[i][\"Status\"].sum()/n)*100,decimals=4)\n",
    "  np_mean_survival_time[i] = ls_simulations[i][\"Survival_time\"].mean()\n",
    "    \n",
    "  #Eliminating X (If not, X causes problems in the loop)\n",
    "  del X\n",
    "\n",
    "time_simulations_4 = timer() - ti   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045e827",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f25d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections = np.zeros(nsim)\n",
    "np_CoxLasso_Power = np.zeros(nsim)\n",
    "np_CoxLasso_FDP = np.zeros(nsim)\n",
    "np_CoxLasso_FD = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_CoxLasso = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations)\n",
    "\n",
    "for i in range(nsim):\n",
    "  fit_coef_vec = ls_coef_CoxLasso[i] \n",
    "  \n",
    "  #Variable selection of the Penalized Cox proportional hazard model  \n",
    "  np_Number_CoxLasso_Rejections[i] = np.sum(fit_coef_vec!= 0)\n",
    "  print(\"Number of non-zero coefficients: {}\".format(np_Number_CoxLasso_Rejections[i]))\n",
    "  np_rejections_CoxLasso = np.where(fit_coef_vec!= 0,1,0)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the Penalized Cox proportional hazard model  \n",
    "  np_CoxLasso_Power[i] = np.around(100*(np.dot(np_rejections_CoxLasso, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_CoxLasso_FDP[i] = np.around(100*np.dot(np_rejections_CoxLasso, ls_beta[i] == 0) / np_rejections_CoxLasso.sum(), decimals=2)\n",
    "  np_CoxLasso_FD[i] = np.around(np.dot(np_rejections_CoxLasso, ls_beta[i] == 0))\n",
    "  print(f\"The Penalized Cox proportional hazards model has discovered {np_CoxLasso_Power[i]}% of the non-nulls with a FDP of {np_CoxLasso_FDP[i]}%\")\n",
    "\n",
    "time_CoxLASSO_4 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173930e",
   "metadata": {},
   "source": [
    "### Estimation of the latent correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b9fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_latentcor = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_latentcor = Parallel(n_jobs=jobs)(delayed(latentcor_estimation)(x,types) for x,types in zip(ls_X,ls_types))\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Relative Frobenius norm\n",
    "  np_Frobenius_norm_latentcor[i]= np.linalg.norm(matrices_latentcor[i] -covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_latentcor_4 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c88070",
   "metadata": {},
   "source": [
    "### Graphical lasso estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_Sigma_hat = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_Sigma_hat = Parallel(n_jobs=jobs)(delayed(glasso_function)(x) for x in matrices_latentcor)\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "    #Relative Frobenius norm\n",
    "    np_Frobenius_norm_Sigma_hat[i]= np.linalg.norm(matrices_Sigma_hat[i]-covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "    \n",
    "time_GraphicalLASSO_4 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee077a4c",
   "metadata": {},
   "source": [
    "### Knockoffs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_simulations_Xk_hat = list(range(nsim))\n",
    "\n",
    "ti = timer() # Initial time for the loop\n",
    "\n",
    "for i in range(nsim):\n",
    "\n",
    "  X = ls_X[i]\n",
    "  df_X = ls_X[i] \n",
    "  types = ls_types[i]\n",
    "  \n",
    "  #From Python to R\n",
    "  %R -i X \n",
    "  %R -i types\n",
    "  %R -i delta_n\n",
    "  \n",
    "  #Transformation of the marginal distribution to normal distribution\n",
    "  %R X_ecdf <- X  \n",
    "  %R X_norm_hat <- X\n",
    "  \n",
    "  #Empirical cumulative distribution function\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i] <- as.vector(ecdf(X[,i])(X[,i])) }}\n",
    "\n",
    "  #For truncation:\n",
    "  #Continuous variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] < delta_n] <- delta_n }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] > (1-delta_n)] <- 1-delta_n }}                                    \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_norm_hat[,i] <- as.vector(qnorm( X_ecdf[,i] ) )}}\n",
    "  #Ordinal variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ X_norm_hat[,i][X[,i]==0] = (-1)}}  \n",
    "\n",
    "  #From R to Python\n",
    "  %R -o X_norm_hat\n",
    "    \n",
    "  #Object for Gaussian knockoffs using the Sigma_hat and method mvr\n",
    "  Gaussian_sampler_hat = knockpy.knockoffs.GaussianSampler(X_norm_hat.to_numpy(), mu=None,\n",
    "                                                           Sigma=matrices_Sigma_hat[i],\n",
    "                                                           method='mvr', verbose=False)\n",
    "  Xk_norm_hat = Gaussian_sampler_hat.sample_knockoffs()  \n",
    "\n",
    "  \n",
    "  #Creating a dataframes from the knockoffs Xk_norm_hat\n",
    "  df_Xk_norm_hat = pd.DataFrame(Xk_norm_hat)\n",
    "\n",
    "  #From Python to R\n",
    "  %R -i df_Xk_norm_hat\n",
    "\n",
    "  #Transformation of Gaussian knockoffs to the original marginal distribution\n",
    "  %R df_Xk_hat <- df_Xk_norm_hat\n",
    "   \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ df_Xk_hat[,i] <- as.vector(quantile(X[,i], probs=pnorm(df_Xk_norm_hat[,i]), type=8)) }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]>=0]= 1}}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]<0]= 0}}\n",
    "  \n",
    "  #From R to Python  \n",
    "  %R -o df_Xk_hat\n",
    "  df_Xk_hat.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "  #Creating the names for the variables in Xk_hat\n",
    "  numbers = np.arange(1,p+1)\n",
    "  kvar_names = ['KVar'+ str(number) for number in numbers]\n",
    "  df_Xk_hat.columns= kvar_names     \n",
    "\n",
    "  #Final dataset\n",
    "  ls_simulations_Xk_hat[i] = pd.concat([ls_simulations[i], df_Xk_hat], axis=1)\n",
    "  \n",
    "time_knockoffs_4 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227f2d1",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazard’s model with lasso penalization for (X,Xk_hat) (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0687087",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "\n",
    "tii = timer()#Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_knockoff_hat = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations_Xk_hat)\n",
    "\n",
    "for i in range(nsim): \n",
    "    np_Number_CoxLasso_Rejections_knockoff_hat[i] = np.sum(ls_coef_knockoff_hat[i]!= 0)\n",
    "\n",
    "time_CoxLASSO_X_Xk_4 = timer() - tii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a378885f",
   "metadata": {},
   "source": [
    "### Wj using the LASSO coeficient difference statistics and threshold rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "np_knockoff_hat_Power = np.zeros(nsim)\n",
    "np_knockoff_hat_FDP = np.zeros(nsim)\n",
    "np_knockoff_hat_FD = np.zeros(nsim)\n",
    "\n",
    "for i in range(nsim): \n",
    "  \n",
    "  #feature important asignation\n",
    "  Z = ls_coef_knockoff_hat[i]\n",
    "\n",
    "  #Wj statistic\n",
    "  pair_W = np.abs(Z[0:p]) - np.abs(Z[p:])\n",
    " \n",
    "  #Threshold selection and variable selection  \n",
    "  threshold = data_dependent_threshhold(W=pair_W, fdr= FDR)\n",
    "  print(\"Threshold for knockoffs \")\n",
    "  print(threshold)\n",
    "  rejections = make_selections(W=pair_W, fdr= FDR)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the knockoff procedure\n",
    "  np_Number_Rejections_knockoff_hat[i] = rejections.sum()\n",
    "  print(\"Number of non-zero knockoff coefficients: {}\".format(np_Number_Rejections_knockoff_hat[i]))\n",
    "  np_knockoff_hat_Power[i] = np.around(100*(np.dot(rejections, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_knockoff_hat_FDP[i] = np.around(100*np.dot(rejections, ls_beta[i] == 0) / rejections.sum(), decimals=2)\n",
    "  np_knockoff_hat_FD[i] = np.around(np.dot(rejections, ls_beta[i] == 0) )\n",
    "  print(f\"The knockoff filter has discovered {np_knockoff_hat_Power[i]}% of the non-nulls with a FDP of {np_knockoff_hat_FDP[i]}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of varying feature\n",
    "np_varying_feature = np.repeat([40],repeats=nsim,axis=0)\n",
    "\n",
    "\n",
    "#Dataframe with simulated data\n",
    "df_simulations_results_4 = pd.DataFrame({'Censoring':np_varying_feature,\n",
    "                                 'Censoring indicator':np_censoring,\n",
    "                                 'Mean survival time':np_mean_survival_time,\n",
    "                                 'Relative Frobenius norm latentcor':np_Frobenius_norm_latentcor,\n",
    "                                 'Relative Frobenius norm Sigma hat':np_Frobenius_norm_Sigma_hat,\n",
    "                     'Number_CoxLasso_Rejections':np_Number_CoxLasso_Rejections, \n",
    "                     'CoxLasso_Power(%)':np_CoxLasso_Power, \n",
    "                     'CoxLasso_FDP(%)':np_CoxLasso_FDP,\n",
    "                     'CoxLasso_FD':np_CoxLasso_FD,\n",
    "                     'Number_CoxLasso_Rejections_knockoff_hat':np_Number_CoxLasso_Rejections_knockoff_hat,\n",
    "                     'Number_Rejections_knockoff_hat':np_Number_Rejections_knockoff_hat, \n",
    "                     'knockoff_hat_Power(%)':np_knockoff_hat_Power, \n",
    "                     'knockoff_hat_FDP(%)':np_knockoff_hat_FDP,\n",
    "                     'knockoff_hat_FD':np_knockoff_hat_FD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ab2c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_simulations_results_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulations_results_4.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e8fb5",
   "metadata": {},
   "source": [
    "# Censoring 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 10004000\n",
    "\n",
    "#Censored upper limit\n",
    "u_max=13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b9587",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Pyhton to R\n",
    "%R -i p\n",
    "%R -i b_by_block\n",
    "%R -i rho\n",
    "\n",
    "%R n_blocks <- p%/%b_by_block\n",
    "%R covMat <- diag(n_blocks) %x% matrix(rho^abs(matrix(1:b_by_block,b_by_block, b_by_block, byrow = TRUE) - matrix(1:b_by_block, b_by_block, b_by_block)), b_by_block, b_by_block)\n",
    "%R diag(covMat) <- 1\n",
    "\n",
    "#From R to Pyhton\n",
    "%R -o covMat\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8eaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the truncated ECFD estimator (Reference___)\n",
    "delta_n = 1/( (4*n**(1/4))*math.sqrt(math.pi*math.log(n)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621bbe1",
   "metadata": {},
   "source": [
    "### Simulations: design matrix X and survival time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in globals():\n",
    "    del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad06d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arrays and listt to save information\n",
    "ls_simulations = list(range(nsim))\n",
    "ls_beta = list(range(nsim))\n",
    "ls_X = list(range(nsim))\n",
    "ls_types = list(range(nsim))\n",
    "np_censoring = np.zeros(nsim)\n",
    "np_mean_survival_time = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Set seed for replication\n",
    "  np.random.seed(i+replication)  #Python\n",
    "  %R -i i\n",
    "  %R -i replication\n",
    "  %R set.seed(i+replication)  #R\n",
    "\n",
    "  #Sigma assignation\n",
    "  Sigma = covMat\n",
    "\n",
    "  #From Python to R \n",
    "  %R -i Sigma\n",
    "  %R -i n\n",
    "  %R -i p_bin\n",
    "  %R -i p  \n",
    "  %R -i p_nonnull_ordinal \n",
    "  %R -i p_nonnull_cont \n",
    "  %R -i alpha_level\n",
    "  %R -i nu_level\n",
    "\n",
    "  #Creation of the vector with correlation information\n",
    "  %R lowerpart <- lower.tri(Sigma)\n",
    "  %R rhos <- Sigma[lowerpart] \n",
    "  %R p_con <- p - p_bin\n",
    "\n",
    "\n",
    "  #Binary and continiuos variables\n",
    "  %R binary <- rep(\"bin\", p_bin)\n",
    "  %R con <- rep(\"con\", p_con)\n",
    "  %R types <- sample(c(binary, con))\n",
    "  \n",
    "  #Simulations of a mixed random vector\n",
    "  %R X_norm_bin <- as.data.frame(gen_data(n = n, types = types, rhos = rhos, copulas=\"no\", XP = NULL, showplot = FALSE)$X)\n",
    "  %R X <- X_norm_bin\n",
    "\n",
    "  if (skew_dist==\"Yes\"):  \n",
    "    #There is seldom an error using the default solver \"NB\" for the function qsn().\n",
    "    #Thus, in case of error, the solver is switched to \"RFB\"\n",
    "    %R for(i in 1:p) {   if(types[i]==\"con\"){ tryCatch({X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level,nu=nu_level))}, error=function(e){X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level, nu=nu_level, solver=\"RFB\"))}) }}\n",
    "   \n",
    "  #Creating a dataframe using the design matrix X\n",
    "  #From R to Pyhton\n",
    "  %R -o X \n",
    "  %R -o types\n",
    "  %R -o p_con\n",
    "  \n",
    "  X.reset_index(drop=True, inplace=True)\n",
    "  ls_X[i] = X\n",
    "  ls_types[i] = types\n",
    "  df_X = pd.DataFrame(X)\n",
    "\n",
    "  #Names for the variables (X)\n",
    "  numbers = np.arange(1,p+1)\n",
    "  var_names = ['Var'+ str(number) for number in numbers]\n",
    "  df_X.columns= var_names\n",
    "\n",
    "  #Survival time simulation\n",
    "\n",
    "  #Extracting the ordinal and continuous variables\n",
    "  %R col_ind_ordinal <- sapply(X, function(col) length(unique(col)) < 4)\n",
    "  %R col_ind_cont <- sapply(X, function(col) length(unique(col)) > 4)\n",
    "  %R col_names <- names(X)\n",
    "  %R col_names_ordinal <- col_names[col_ind_ordinal]\n",
    "  %R col_names_cont <- col_names[col_ind_cont]  \n",
    "\n",
    "  #Variables and coefficients nonnull\n",
    "  if (p_nonnull_ordinal==0):\n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE))\n",
    "  else:\n",
    "      #If there are ordinal nonnull variables the proportion of nonnull continuous and nonnull ordinal\n",
    "      #remains the same between diferent simulated data sets  \n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE), sample(col_names_ordinal, size=p_nonnull_ordinal, replace=FALSE))\n",
    "   \n",
    "  %R ind_betas <- as.numeric(substr(col_nonnull,start=2,stop=4))\n",
    "  %R -o ind_betas \n",
    "  ind_betas_sort = np.sort(ind_betas.astype(int))\n",
    "  \n",
    "  #betas and magnitud\n",
    "  beta = np.zeros(p,dtype=float)\n",
    "  beta[ind_betas_sort-1] = beta_coef\n",
    "  ls_beta[i] = beta\n",
    "\n",
    "  #Survival time simulations (Bender et al. 2006)\n",
    "  t = ( - (np.log(np.random.uniform(low=0.0, high=1.0, size=n)) )/( lambda_T*np.exp( np.dot(X, beta) ) ))**(1/nu_T)\n",
    "\n",
    "  #Censored time and Censored indicator\n",
    "  t_cens = np.random.uniform(0, u_max, size=n) #With Upper=70 --> 34% of censoring\n",
    "  I_cens = np.where(t<= t_cens, 1, 0)\n",
    "\n",
    "  #Observed time \n",
    "  t_obs = np.minimum(t, t_cens)    \n",
    "\n",
    "  #Creating the dataframe with survival information\n",
    "  df_Y = pd.DataFrame({ 'Status':I_cens,'Survival_time':t_obs})\n",
    "  \n",
    "  #Data frame with simulated data (Y,X)\n",
    "  ls_simulations[i] = pd.concat([df_Y,df_X], axis=1)\n",
    "\n",
    "  #Saving information\n",
    "  np_censoring[i] = np.around((1-ls_simulations[i][\"Status\"].sum()/n)*100,decimals=4)\n",
    "  np_mean_survival_time[i] = ls_simulations[i][\"Survival_time\"].mean()\n",
    "    \n",
    "  #Eliminating X (If not, X causes problems in the loop)\n",
    "  del X\n",
    "\n",
    "time_simulations_5 = timer() - ti     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8100db",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ef862",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections = np.zeros(nsim)\n",
    "np_CoxLasso_Power = np.zeros(nsim)\n",
    "np_CoxLasso_FDP = np.zeros(nsim)\n",
    "np_CoxLasso_FD = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_CoxLasso = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations)\n",
    "\n",
    "for i in range(nsim):\n",
    "  fit_coef_vec = ls_coef_CoxLasso[i] \n",
    "  \n",
    "  #Variable selection of the Penalized Cox proportional hazard model  \n",
    "  np_Number_CoxLasso_Rejections[i] = np.sum(fit_coef_vec!= 0)\n",
    "  print(\"Number of non-zero coefficients: {}\".format(np_Number_CoxLasso_Rejections[i]))\n",
    "  np_rejections_CoxLasso = np.where(fit_coef_vec!= 0,1,0)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the Penalized Cox proportional hazard model  \n",
    "  np_CoxLasso_Power[i] = np.around(100*(np.dot(np_rejections_CoxLasso, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_CoxLasso_FDP[i] = np.around(100*np.dot(np_rejections_CoxLasso, ls_beta[i] == 0) / np_rejections_CoxLasso.sum(), decimals=2)\n",
    "  np_CoxLasso_FD[i] = np.around(np.dot(np_rejections_CoxLasso, ls_beta[i] == 0))\n",
    "  print(f\"The Penalized Cox proportional hazards model has discovered {np_CoxLasso_Power[i]}% of the non-nulls with a FDP of {np_CoxLasso_FDP[i]}%\")\n",
    "    \n",
    "time_CoxLASSO_5 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34414c",
   "metadata": {},
   "source": [
    "### Estimation of the latent correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_latentcor = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_latentcor = Parallel(n_jobs=jobs)(delayed(latentcor_estimation)(x,types) for x,types in zip(ls_X,ls_types))\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Relative Frobenius norm\n",
    "  np_Frobenius_norm_latentcor[i]= np.linalg.norm(matrices_latentcor[i] -covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_latentcor_5 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa74ac",
   "metadata": {},
   "source": [
    "### Graphical lasso estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_Sigma_hat = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_Sigma_hat = Parallel(n_jobs=jobs)(delayed(glasso_function)(x) for x in matrices_latentcor)\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "    #Relative Frobenius norm\n",
    "    np_Frobenius_norm_Sigma_hat[i]= np.linalg.norm(matrices_Sigma_hat[i]-covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_GraphicalLASSO_5 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e07bb8",
   "metadata": {},
   "source": [
    "### Knockoffs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_simulations_Xk_hat = list(range(nsim))\n",
    "\n",
    "ti = timer() #Initial time for the loop\n",
    "\n",
    "for i in range(nsim):\n",
    "\n",
    "  X = ls_X[i]\n",
    "  df_X = ls_X[i] \n",
    "  types = ls_types[i]\n",
    "  \n",
    "  #From Python to R\n",
    "  %R -i X \n",
    "  %R -i types\n",
    "  %R -i delta_n\n",
    "  \n",
    "  #Transformation of the marginal distribution to normal distribution\n",
    "  %R X_ecdf <- X  \n",
    "  %R X_norm_hat <- X\n",
    "  \n",
    "  #Empirical cumulative distribution function\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i] <- as.vector(ecdf(X[,i])(X[,i])) }}\n",
    "\n",
    "  #For truncation:\n",
    "  #Continuous variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] < delta_n] <- delta_n }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] > (1-delta_n)] <- 1-delta_n }}                                    \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_norm_hat[,i] <- as.vector(qnorm( X_ecdf[,i] ) )}}\n",
    "  #Ordinal variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ X_norm_hat[,i][X[,i]==0] = (-1)}}  \n",
    "\n",
    "  #From R to Python\n",
    "  %R -o X_norm_hat\n",
    "    \n",
    "  #Object for Gaussian knockoffs using the Sigma_hat and method mvr\n",
    "  Gaussian_sampler_hat = knockpy.knockoffs.GaussianSampler(X_norm_hat.to_numpy(), mu=None,\n",
    "                                                           Sigma=matrices_Sigma_hat[i],\n",
    "                                                           method='mvr', verbose=False)\n",
    "  Xk_norm_hat = Gaussian_sampler_hat.sample_knockoffs()  \n",
    "\n",
    "  \n",
    "  #Creating a dataframes from the knockoffs Xk_norm_hat\n",
    "  df_Xk_norm_hat = pd.DataFrame(Xk_norm_hat)\n",
    "\n",
    "  #From Python to R\n",
    "  %R -i df_Xk_norm_hat\n",
    "\n",
    "  #Transformation of Gaussian knockoffs to the original marginal distribution\n",
    "  %R df_Xk_hat <- df_Xk_norm_hat\n",
    "   \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ df_Xk_hat[,i] <- as.vector(quantile(X[,i], probs=pnorm(df_Xk_norm_hat[,i]), type=8)) }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]>=0]= 1}}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]<0]= 0}}\n",
    "  \n",
    "  #From R to Python  \n",
    "  %R -o df_Xk_hat\n",
    "  df_Xk_hat.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "  #Creating the names for the variables in X\n",
    "  numbers = np.arange(1,p+1)\n",
    "  kvar_names = ['KVar'+ str(number) for number in numbers]\n",
    "  df_Xk_hat.columns= kvar_names     \n",
    "\n",
    "  #Final dataset\n",
    "  ls_simulations_Xk_hat[i] = pd.concat([ls_simulations[i], df_Xk_hat], axis=1)\n",
    "  \n",
    "time_knockoffs_5 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f513a8",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization for (X,Xk_hat) (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "\n",
    "tii = timer()#Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_knockoff_hat = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations_Xk_hat)\n",
    "\n",
    "for i in range(nsim): \n",
    "    np_Number_CoxLasso_Rejections_knockoff_hat[i] = np.sum(ls_coef_knockoff_hat[i]!= 0)\n",
    "\n",
    "time_CoxLASSO_X_Xk_5 = timer() - tii "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c18e3",
   "metadata": {},
   "source": [
    "### Wj using the LASSO coeficient difference statistics and threshold rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29097d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "np_knockoff_hat_Power = np.zeros(nsim)\n",
    "np_knockoff_hat_FDP = np.zeros(nsim)\n",
    "np_knockoff_hat_FD = np.zeros(nsim)\n",
    "\n",
    "for i in range(nsim): \n",
    "  \n",
    "  #feature important asignation\n",
    "  Z = ls_coef_knockoff_hat[i]\n",
    "\n",
    "  #Wj statistic\n",
    "  pair_W = np.abs(Z[0:p]) - np.abs(Z[p:])\n",
    "\n",
    "  #Threshold selection and variable selection\n",
    "  threshold = data_dependent_threshhold(W=pair_W, fdr= FDR)\n",
    "  print(\"Threshold for knockoffs \")\n",
    "  print(threshold)\n",
    "  rejections = make_selections(W=pair_W, fdr= FDR)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the knockoff procedure\n",
    "  np_Number_Rejections_knockoff_hat[i] = rejections.sum()\n",
    "  print(\"Number of non-zero knockoff coefficients: {}\".format(np_Number_Rejections_knockoff_hat[i]))\n",
    "  np_knockoff_hat_Power[i] = np.around(100*(np.dot(rejections, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_knockoff_hat_FDP[i] = np.around(100*np.dot(rejections, ls_beta[i] == 0) / rejections.sum(), decimals=2)\n",
    "  np_knockoff_hat_FD[i] = np.around(np.dot(rejections, ls_beta[i] == 0) )\n",
    "  print(f\"The knockoff filter has discovered {np_knockoff_hat_Power[i]}% of the non-nulls with a FDP of {np_knockoff_hat_FDP[i]}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of varying feature\n",
    "np_varying_feature = np.repeat([50],repeats=nsim,axis=0)\n",
    "\n",
    "\n",
    "#Dataframe with simulated data\n",
    "df_simulations_results_5 = pd.DataFrame({'Censoring':np_varying_feature,\n",
    "                                 'Censoring indicator':np_censoring,\n",
    "                                 'Mean survival time':np_mean_survival_time,\n",
    "                                 'Relative Frobenius norm latentcor':np_Frobenius_norm_latentcor,\n",
    "                                 'Relative Frobenius norm Sigma hat':np_Frobenius_norm_Sigma_hat,\n",
    "                     'Number_CoxLasso_Rejections':np_Number_CoxLasso_Rejections, \n",
    "                     'CoxLasso_Power(%)':np_CoxLasso_Power, \n",
    "                     'CoxLasso_FDP(%)':np_CoxLasso_FDP,\n",
    "                     'CoxLasso_FD':np_CoxLasso_FD,\n",
    "                     'Number_CoxLasso_Rejections_knockoff_hat':np_Number_CoxLasso_Rejections_knockoff_hat,\n",
    "                     'Number_Rejections_knockoff_hat':np_Number_Rejections_knockoff_hat, \n",
    "                     'knockoff_hat_Power(%)':np_knockoff_hat_Power, \n",
    "                     'knockoff_hat_FDP(%)':np_knockoff_hat_FDP,\n",
    "                     'knockoff_hat_FD':np_knockoff_hat_FD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8437481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulations_results_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulations_results_5.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c80f2",
   "metadata": {},
   "source": [
    "# Censoring 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 10004000\n",
    "\n",
    "#Censored upper limit\n",
    "u_max=6.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d851d16",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Pyhton to R\n",
    "%R -i p\n",
    "%R -i b_by_block\n",
    "%R -i rho\n",
    "\n",
    "%R n_blocks <- p%/%b_by_block\n",
    "%R covMat <- diag(n_blocks) %x% matrix(rho^abs(matrix(1:b_by_block,b_by_block, b_by_block, byrow = TRUE) - matrix(1:b_by_block, b_by_block, b_by_block)), b_by_block, b_by_block)\n",
    "%R diag(covMat) <- 1\n",
    "\n",
    "#From R to Pyhton\n",
    "%R -o covMat\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the truncated ECFD estimator (Reference___)\n",
    "delta_n = 1/( (4*n**(1/4))*math.sqrt(math.pi*math.log(n)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1524",
   "metadata": {},
   "source": [
    "### Simulations: design matrix X and survival time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in globals():\n",
    "    del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbdc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arrays and listt to save information\n",
    "ls_simulations = list(range(nsim))\n",
    "ls_beta = list(range(nsim))\n",
    "ls_X = list(range(nsim))\n",
    "ls_types = list(range(nsim))\n",
    "np_censoring = np.zeros(nsim)\n",
    "np_mean_survival_time = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Set seed for replication\n",
    "  np.random.seed(i+replication)  #Python\n",
    "  %R -i i\n",
    "  %R -i replication\n",
    "  %R set.seed(i+replication)  #R\n",
    "\n",
    "  #Sigma assignation\n",
    "  Sigma = covMat\n",
    "\n",
    "  #From Python to R \n",
    "  %R -i Sigma\n",
    "  %R -i n\n",
    "  %R -i p_bin\n",
    "  %R -i p  \n",
    "  %R -i p_nonnull_ordinal \n",
    "  %R -i p_nonnull_cont \n",
    "  %R -i alpha_level\n",
    "  %R -i nu_level\n",
    "\n",
    "  #Creation of the vector with correlation information\n",
    "  %R lowerpart <- lower.tri(Sigma)\n",
    "  %R rhos <- Sigma[lowerpart] \n",
    "  %R p_con <- p - p_bin\n",
    "\n",
    "\n",
    "  #Binary and continiuos variables\n",
    "  %R binary <- rep(\"bin\", p_bin)\n",
    "  %R con <- rep(\"con\", p_con)\n",
    "  %R types <- sample(c(binary, con))\n",
    "  \n",
    "  #Simulations of a mixed random vector\n",
    "  %R X_norm_bin <- as.data.frame(gen_data(n = n, types = types, rhos = rhos, copulas=\"no\", XP = NULL, showplot = FALSE)$X)\n",
    "  %R X <- X_norm_bin\n",
    "\n",
    "  if (skew_dist==\"Yes\"):  \n",
    "    #There is seldom an error using the default solver \"NB\" for the function qsn().\n",
    "    #Thus, in case of error, the solver is switched to \"RFB\"\n",
    "    %R for(i in 1:p) {   if(types[i]==\"con\"){ tryCatch({X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level,nu=nu_level))}, error=function(e){X[,i] <-as.vector(qst(pnorm(X_norm_bin[,i], mean=0,sd=1),xi=0, omega=1,alpha=alpha_level, nu=nu_level, solver=\"RFB\"))}) }}\n",
    "   \n",
    "  #Creating a dataframe using the design matrix X\n",
    "  #From R to Pyhton\n",
    "  %R -o X \n",
    "  %R -o types\n",
    "  %R -o p_con\n",
    "  \n",
    "  X.reset_index(drop=True, inplace=True)\n",
    "  ls_X[i] = X\n",
    "  ls_types[i] = types\n",
    "  df_X = pd.DataFrame(X)\n",
    "\n",
    "  #Names for the variables (X)\n",
    "  numbers = np.arange(1,p+1)\n",
    "  var_names = ['Var'+ str(number) for number in numbers]\n",
    "  df_X.columns= var_names\n",
    "\n",
    "  #Survival time simulation\n",
    "\n",
    "  #Extracting the ordinal and continuous variables\n",
    "  %R col_ind_ordinal <- sapply(X, function(col) length(unique(col)) < 4)\n",
    "  %R col_ind_cont <- sapply(X, function(col) length(unique(col)) > 4)\n",
    "  %R col_names <- names(X)\n",
    "  %R col_names_ordinal <- col_names[col_ind_ordinal]\n",
    "  %R col_names_cont <- col_names[col_ind_cont]  \n",
    "\n",
    "  #Variables and coefficients nonnull\n",
    "  if (p_nonnull_ordinal==0):\n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE))\n",
    "  else:\n",
    "      #If there are ordinal nonnull variables the proportion of nonnull continuous and nonnull ordinal\n",
    "      #remains the same between diferent simulated data sets  \n",
    "      %R col_nonnull <- c(sample(col_names_cont, size=p_nonnull_cont, replace=FALSE), sample(col_names_ordinal, size=p_nonnull_ordinal, replace=FALSE))\n",
    "   \n",
    "  %R ind_betas <- as.numeric(substr(col_nonnull,start=2,stop=4))\n",
    "  %R -o ind_betas \n",
    "  ind_betas_sort = np.sort(ind_betas.astype(int))\n",
    "  \n",
    "  #betas and magnitud\n",
    "  beta = np.zeros(p,dtype=float)\n",
    "  beta[ind_betas_sort-1] = beta_coef\n",
    "  ls_beta[i] = beta\n",
    "\n",
    "  #Survival time simulations (Bender et al. 2006)\n",
    "  t = ( - (np.log(np.random.uniform(low=0.0, high=1.0, size=n)) )/( lambda_T*np.exp( np.dot(X, beta) ) ))**(1/nu_T)\n",
    "\n",
    "  #Censored time and Censored indicator\n",
    "  t_cens = np.random.uniform(0, u_max, size=n) #With Upper=70 --> 34% of censoring\n",
    "  I_cens = np.where(t<= t_cens, 1, 0)\n",
    "\n",
    "  #Observed time \n",
    "  t_obs = np.minimum(t, t_cens)    \n",
    "\n",
    "  #Creating the dataframe with survival information\n",
    "  df_Y = pd.DataFrame({ 'Status':I_cens,'Survival_time':t_obs})\n",
    "  \n",
    "  #Data frame with simulated data (Y,X)\n",
    "  ls_simulations[i] = pd.concat([df_Y,df_X], axis=1)\n",
    "\n",
    "  #Saving information\n",
    "  np_censoring[i] = np.around((1-ls_simulations[i][\"Status\"].sum()/n)*100,decimals=4)\n",
    "  np_mean_survival_time[i] = ls_simulations[i][\"Survival_time\"].mean()\n",
    "    \n",
    "  #Eliminating X (If not, X causes problems in the loop)\n",
    "  del X\n",
    "\n",
    "time_simulations_6 = timer() - ti     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0575e",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd272542",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections = np.zeros(nsim)\n",
    "np_CoxLasso_Power = np.zeros(nsim)\n",
    "np_CoxLasso_FDP = np.zeros(nsim)\n",
    "np_CoxLasso_FD = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_CoxLasso = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations)\n",
    "\n",
    "for i in range(nsim):\n",
    "  fit_coef_vec = ls_coef_CoxLasso[i] \n",
    "  \n",
    "  #Variable selection of the Penalized Cox proportional hazard model  \n",
    "  np_Number_CoxLasso_Rejections[i] = np.sum(fit_coef_vec!= 0)\n",
    "  print(\"Number of non-zero coefficients: {}\".format(np_Number_CoxLasso_Rejections[i]))\n",
    "  np_rejections_CoxLasso = np.where(fit_coef_vec!= 0,1,0)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the Penalized Cox proportional hazard model  \n",
    "  np_CoxLasso_Power[i] = np.around(100*(np.dot(np_rejections_CoxLasso, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_CoxLasso_FDP[i] = np.around(100*np.dot(np_rejections_CoxLasso, ls_beta[i] == 0) / np_rejections_CoxLasso.sum(), decimals=2)\n",
    "  np_CoxLasso_FD[i] = np.around(np.dot(np_rejections_CoxLasso, ls_beta[i] == 0))\n",
    "  print(f\"The Penalized Cox proportional hazards model has discovered {np_CoxLasso_Power[i]}% of the non-nulls with a FDP of {np_CoxLasso_FDP[i]}%\")\n",
    "    \n",
    "time_CoxLASSO_6 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05fb8fe",
   "metadata": {},
   "source": [
    "### Estimation of the latent correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_latentcor = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_latentcor = Parallel(n_jobs=jobs)(delayed(latentcor_estimation)(x,types) for x,types in zip(ls_X,ls_types))\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "  #Relative Frobenius norm\n",
    "  np_Frobenius_norm_latentcor[i]= np.linalg.norm(matrices_latentcor[i] -covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_latentcor_6 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a7182",
   "metadata": {},
   "source": [
    "### Graphical lasso estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0032f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Frobenius_norm_Sigma_hat = np.zeros(nsim)\n",
    "\n",
    "ti = timer() #Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "matrices_Sigma_hat = Parallel(n_jobs=jobs)(delayed(glasso_function)(x) for x in matrices_latentcor)\n",
    "\n",
    "for i in range(nsim):\n",
    "    \n",
    "    #Relative Frobenius norm\n",
    "    np_Frobenius_norm_Sigma_hat[i]= np.linalg.norm(matrices_Sigma_hat[i]-covMat, 'fro')/np.linalg.norm(covMat, 'fro')\n",
    "\n",
    "time_GraphicalLASSO_6 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0aa65",
   "metadata": {},
   "source": [
    "### Knockoffs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ec072",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_simulations_Xk_hat = list(range(nsim))\n",
    "\n",
    "ti = timer() #Initial time for the loop\n",
    "\n",
    "for i in range(nsim):\n",
    "\n",
    "  X = ls_X[i]\n",
    "  df_X = ls_X[i] \n",
    "  types = ls_types[i]\n",
    "  \n",
    "  #From Python to R\n",
    "  %R -i X \n",
    "  %R -i types\n",
    "  %R -i delta_n\n",
    "  \n",
    "  #Transformation of the marginal distribution to normal distribution\n",
    "  %R X_ecdf <- X  \n",
    "  %R X_norm_hat <- X\n",
    "  \n",
    "  #Empirical cumulative distribution function\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i] <- as.vector(ecdf(X[,i])(X[,i])) }}\n",
    "\n",
    "  #For truncation:\n",
    "  #Continuous variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] < delta_n] <- delta_n }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_ecdf[,i][ X_ecdf[,i] > (1-delta_n)] <- 1-delta_n }}                                    \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ X_norm_hat[,i] <- as.vector(qnorm( X_ecdf[,i] ) )}}\n",
    "  #Ordinal variables\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ X_norm_hat[,i][X[,i]==0] = (-1)}}  \n",
    "\n",
    "  #From R to Python\n",
    "  %R -o X_norm_hat\n",
    "    \n",
    "  #Object for Gaussian knockoffs using the Sigma_hat and method mvr\n",
    "  Gaussian_sampler_hat = knockpy.knockoffs.GaussianSampler(X_norm_hat.to_numpy(), mu=None,\n",
    "                                                           Sigma=matrices_Sigma_hat[i],\n",
    "                                                           method='mvr', verbose=False)\n",
    "  Xk_norm_hat = Gaussian_sampler_hat.sample_knockoffs()  \n",
    "\n",
    "  \n",
    "  #Creating a dataframes from the knockoffs Xk_norm_hat\n",
    "  df_Xk_norm_hat = pd.DataFrame(Xk_norm_hat)\n",
    "\n",
    "  #From Python to R\n",
    "  %R -i df_Xk_norm_hat\n",
    "\n",
    "  #Transformation of Gaussian knockoffs to the original marginal distribution\n",
    "  %R df_Xk_hat <- df_Xk_norm_hat\n",
    "   \n",
    "  %R for(i in 1:p) {   if(types[i]==\"con\"){ df_Xk_hat[,i] <- as.vector(quantile(X[,i], probs=pnorm(df_Xk_norm_hat[,i]), type=8)) }}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]>=0]= 1}}\n",
    "  %R for(i in 1:p) {   if(types[i]==\"bin\"){ df_Xk_hat[,i][df_Xk_norm_hat[,i]<0]= 0}}\n",
    "  \n",
    "  #From R to Python  \n",
    "  %R -o df_Xk_hat\n",
    "  df_Xk_hat.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "  #Creating the names for the variables in X\n",
    "  numbers = np.arange(1,p+1)\n",
    "  kvar_names = ['KVar'+ str(number) for number in numbers]\n",
    "  df_Xk_hat.columns= kvar_names     \n",
    "\n",
    "  #Final dataset\n",
    "  ls_simulations_Xk_hat[i] = pd.concat([ls_simulations[i], df_Xk_hat], axis=1)\n",
    "  \n",
    "time_knockoffs_6 = timer() - ti "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905deba8",
   "metadata": {},
   "source": [
    "### Cox’s proportional hazards model with lasso penalization for (X,Xk_hat) (glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d57b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_CoxLasso_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "\n",
    "tii = timer()#Initial time\n",
    "\n",
    "#Parallel code with Joblib\n",
    "ls_coef_knockoff_hat = Parallel(n_jobs=jobs)(delayed(lasso_glmnet)(x) for x in ls_simulations_Xk_hat)\n",
    "\n",
    "for i in range(nsim): \n",
    "    np_Number_CoxLasso_Rejections_knockoff_hat[i] = np.sum(ls_coef_knockoff_hat[i]!= 0)\n",
    "\n",
    "time_CoxLASSO_X_Xk_6 = timer() - tii "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c268e6d",
   "metadata": {},
   "source": [
    "### Wj using the LASSO coeficient difference statistics and threshold rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22bba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_Number_Rejections_knockoff_hat = np.zeros(nsim)\n",
    "np_knockoff_hat_Power = np.zeros(nsim)\n",
    "np_knockoff_hat_FDP = np.zeros(nsim)\n",
    "np_knockoff_hat_FD = np.zeros(nsim)\n",
    "\n",
    "for i in range(nsim): \n",
    "  \n",
    "  #feature important asignation\n",
    "  Z = ls_coef_knockoff_hat[i]\n",
    "\n",
    "  #Wj statistic\n",
    "  pair_W = np.abs(Z[0:p]) - np.abs(Z[p:])\n",
    "\n",
    "  #Threshold selection and variable selection\n",
    "  threshold = data_dependent_threshhold(W=pair_W, fdr= FDR)\n",
    "  print(\"Threshold for knockoffs \")\n",
    "  print(threshold)\n",
    "  rejections = make_selections(W=pair_W, fdr= FDR)\n",
    "\n",
    "  #Printing and saving the Power and FDP of the knockoff procedure\n",
    "  np_Number_Rejections_knockoff_hat[i] = rejections.sum()\n",
    "  print(\"Number of non-zero knockoff coefficients: {}\".format(np_Number_Rejections_knockoff_hat[i]))\n",
    "  np_knockoff_hat_Power[i] = np.around(100*(np.dot(rejections, ls_beta[i] != 0) / (ls_beta[i] != 0).sum()), decimals=2)\n",
    "  np_knockoff_hat_FDP[i] = np.around(100*np.dot(rejections, ls_beta[i] == 0) / rejections.sum(), decimals=2)\n",
    "  np_knockoff_hat_FD[i] = np.around(np.dot(rejections, ls_beta[i] == 0) )\n",
    "  print(f\"The knockoff filter has discovered {np_knockoff_hat_Power[i]}% of the non-nulls with a FDP of {np_knockoff_hat_FDP[i]}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of varying feature\n",
    "np_varying_feature = np.repeat([60],repeats=nsim,axis=0)\n",
    "\n",
    "\n",
    "#Dataframe with simulated data\n",
    "df_simulations_results_6 = pd.DataFrame({'Censoring':np_varying_feature,\n",
    "                                 'Censoring indicator':np_censoring,\n",
    "                                 'Mean survival time':np_mean_survival_time,\n",
    "                                 'Relative Frobenius norm latentcor':np_Frobenius_norm_latentcor,\n",
    "                                 'Relative Frobenius norm Sigma hat':np_Frobenius_norm_Sigma_hat,\n",
    "                     'Number_CoxLasso_Rejections':np_Number_CoxLasso_Rejections, \n",
    "                     'CoxLasso_Power(%)':np_CoxLasso_Power, \n",
    "                     'CoxLasso_FDP(%)':np_CoxLasso_FDP,\n",
    "                     'CoxLasso_FD':np_CoxLasso_FD,\n",
    "                     'Number_CoxLasso_Rejections_knockoff_hat':np_Number_CoxLasso_Rejections_knockoff_hat,\n",
    "                     'Number_Rejections_knockoff_hat':np_Number_Rejections_knockoff_hat, \n",
    "                     'knockoff_hat_Power(%)':np_knockoff_hat_Power, \n",
    "                     'knockoff_hat_FDP(%)':np_knockoff_hat_FDP,\n",
    "                     'knockoff_hat_FD':np_knockoff_hat_FD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c51c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulations_results_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulations_results_6.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee95b43",
   "metadata": {},
   "source": [
    "### Time to run all the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c24a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_final = timer()\n",
    "\n",
    "print('Time (hrs) taken to run all is:',round((t_final-t_initial)/3600,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c039e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run time of the different loops\n",
    "\n",
    "time_simulations = time_simulations_1 +time_simulations_2 + time_simulations_3 +time_simulations_4 + time_simulations_5 +time_simulations_6\n",
    "print('Time (hrs) taken to create design matriz X and survival time t:',round(time_simulations/3600,4))\n",
    "\n",
    "time_CoxLASSO = time_CoxLASSO_1 +time_CoxLASSO_2 + time_CoxLASSO_3 +time_CoxLASSO_4 + time_CoxLASSO_5 + time_CoxLASSO_6  \n",
    "print('Time (hrs) taken to run Cox’s proportional hazard’s model with LASSO penalization',round(time_CoxLASSO/3600,4))\n",
    "\n",
    "time_latentcor = time_latentcor_1 +time_latentcor_2 + time_latentcor_3 +time_latentcor_4 + time_latentcor_5 +time_latentcor_6 \n",
    "print('Time (hrs) taken to run latent correlation matrix estimation',round(time_latentcor/3600,4))\n",
    "\n",
    "time_GraphicalLASSO = time_GraphicalLASSO_1 +time_GraphicalLASSO_2 + time_GraphicalLASSO_3 +time_GraphicalLASSO_4 + time_GraphicalLASSO_5 + time_GraphicalLASSO_6  \n",
    "print('Time (hrs) taken to run Graphical LASSO',round(time_GraphicalLASSO/3600,4))\n",
    "\n",
    "time_knockoffs = time_knockoffs_1 +time_knockoffs_2 + time_knockoffs_3 +time_knockoffs_4 +time_knockoffs_5 + time_knockoffs_6 \n",
    "print('Time (hrs) taken to sample knockoffs',round(time_knockoffs/3600,4))\n",
    "\n",
    "\n",
    "time_CoxLASSO_X_Xk = time_CoxLASSO_X_Xk_1 +time_CoxLASSO_X_Xk_2 + time_CoxLASSO_X_Xk_3 +time_CoxLASSO_X_Xk_4 + time_CoxLASSO_X_Xk_5 + time_CoxLASSO_X_Xk_6  \n",
    "print('Time (hrs) taken to run Cox’s proportional hazard’s model with LASSO penalization for (X,Xk)',round(time_CoxLASSO_X_Xk/3600,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7656f3",
   "metadata": {},
   "source": [
    "# Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4ab10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data frame with \n",
    "df_simulations_results = pd.concat([df_simulations_results_1,df_simulations_results_2,df_simulations_results_3,df_simulations_results_4,df_simulations_results_5,df_simulations_results_6], axis=0)\n",
    "df_simulations_results.reset_index(drop=True, inplace=True)\n",
    "df_simulations_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ba945",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df_simulations_results[[\"Censoring\",\"Censoring indicator\",\"CoxLasso_Power(%)\", \"CoxLasso_FDP(%)\", \"knockoff_hat_Power(%)\", \"knockoff_hat_FDP(%)\"]].groupby(\"Censoring\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b66199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2045e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the results to csv file\n",
    "results.to_csv('results_n300_p300_varying_censoring_7sep22.csv')\n",
    "\n",
    "#Saving the simulation_results to csv file\n",
    "df_simulations_results.to_csv('Simulation_results_n300_p300_censoring_amplitude_7sep22.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc59925",
   "metadata": {},
   "source": [
    "### Plots of Average Power and FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e70fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting the font Arial\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "#Average power and FDR plots side-by-side\n",
    "\n",
    "x_points = np.array(results.index)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#fig.suptitle('Varying censoring',fontsize=18)\n",
    "\n",
    "\n",
    "#Figure size\n",
    "mm = 1/25.4\n",
    "fig.set_figwidth(174*mm)\n",
    "fig.set_figheight(80*mm)\n",
    "\n",
    "y1_points = np.array(results[\"CoxLasso_Power(%)\"])\n",
    "y2_points = np.array(results[\"knockoff_hat_Power(%)\"])\n",
    "for i, j in zip(x_points,y1_points):\n",
    "    ax1.text(i, j+3.5, \"{:.1f}\".format(j), ha=\"center\")\n",
    "for i, j in zip(x_points,y2_points):\n",
    "    ax1.text(i, j-8, \"{:.1f}\".format(j), ha=\"center\")\n",
    "    \n",
    "ax1.plot(x_points, y1_points,  marker = 's', label=\"Lasso Cox\", linestyle='dashed')\n",
    "ax1.plot(x_points, y2_points, marker = 'o',label=\"LGCK-LCD\")\n",
    "ax1.set_ylim(0, 112)\n",
    "ax1.set_xlim(6, 64)\n",
    "ax1.set_xticks(x_points)\n",
    "ax1.legend(loc=\"lower left\")\n",
    "ax1.set_xlabel(\"Censoring rate (%)\",fontname='Arial')\n",
    "ax1.set_ylabel(\"Average power (%)\",fontname='Arial')\n",
    "\n",
    "\n",
    "y1_points = np.array(results[\"CoxLasso_FDP(%)\"])\n",
    "y2_points = np.array(results[\"knockoff_hat_FDP(%)\"])\n",
    "\n",
    "for i, j in zip(x_points,y1_points):\n",
    "    ax2.text(i, j+3.5, \"{:.1f}\".format(j), ha=\"center\")\n",
    "for i, j in zip(x_points,y2_points):\n",
    "    ax2.text(i, j+3, \"{:.1f}\".format(j), ha=\"center\")\n",
    "\n",
    "ax2.plot(x_points, y1_points,  marker = 's', label=\"Lasso Cox\", linestyle='dashed')\n",
    "ax2.plot(x_points, y2_points, marker = 'o',label=\"LGCK-LCD\")\n",
    "ax2.set_ylim(0, 112)\n",
    "ax2.set_xlim(6, 64)\n",
    "ax2.set_xticks(x_points)\n",
    "ax2.legend(loc=\"upper left\")\n",
    "ax2.set_xlabel(\"Censoring rate (%)\",fontname='Arial')\n",
    "ax2.set_ylabel(\"FDR (%)\",fontname='Arial')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"Fig4.eps\", format=\"eps\", dpi=1200)\n",
    "fig.savefig(\"t_n300_p300_Varying_censoring_7sep22.jpg\", format=\"jpg\", dpi=300)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
